{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Femellistes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrapping les postes de blogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour récupérer les informations d'une page donnée\n",
    "def scrape_page(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        articles = soup.find_all('div', class_='blog-basic-grid--text')\n",
    "        for article in articles:\n",
    "            title = article.find('h1', class_='blog-title').text.strip()\n",
    "            author = article.find('span', class_='blog-author').text.strip()\n",
    "            date = article.find('time', class_='blog-date').text.strip()\n",
    "            categories = [cat.text.strip() for cat in article.find_all('a', class_='blog-categories')]\n",
    "            excerpt = article.find('div', class_='blog-excerpt').text.strip()\n",
    "            read_more_link = article.find('a', class_='blog-more-link')['href']\n",
    "\n",
    "            article_url = f\"https://www.femelliste.com{read_more_link}\"\n",
    "            article_response = requests.get(article_url)\n",
    "            if article_response.status_code == 200:\n",
    "                article_soup = BeautifulSoup(article_response.text, 'html.parser')\n",
    "                main_content = article_soup.find('div', {'data-content-field': 'main-content'})\n",
    "                full_text = main_content.get_text(strip=True)\n",
    "            else:\n",
    "                full_text = \"Échec de la requête pour l'article.\"\n",
    "\n",
    "            writer.writerow({\n",
    "                'Titre': title,\n",
    "                'Auteur': author,\n",
    "                'Date': date,\n",
    "                'Catégories': ', '.join(categories),\n",
    "                'Extrait': excerpt,\n",
    "                'Texte Complet': full_text,\n",
    "                'URL Complète': article_url\n",
    "            })\n",
    "    else:\n",
    "        print(f\"Échec de la requête avec le code d'état: {response.status_code}\")\n",
    "\n",
    "# Ouvrir un fichier TSV en mode écriture avec un délimiteur tab\n",
    "with open('./text/femelliste_articles.tsv', 'w', newline='', encoding='utf-8') as tsvfile:\n",
    "    fieldnames = ['Titre', 'Auteur', 'Date', 'Catégories', 'Extrait', 'Texte Complet', 'URL Complète']\n",
    "    writer = csv.DictWriter(tsvfile, fieldnames=fieldnames, delimiter='\\t')\n",
    "    writer.writeheader()\n",
    "\n",
    "    base_url = \"https://www.femelliste.com/articles-femellisme-feminisme\"\n",
    "\n",
    "    while True:\n",
    "        current_url = f\"{base_url}\"\n",
    "        scrape_page(current_url)\n",
    "\n",
    "        # Vérifier la présence du lien \"Billets plus anciens\"\n",
    "        older_button = BeautifulSoup(requests.get(current_url).text, 'html.parser').find('div', class_='older')\n",
    "        \n",
    "        # Vérifier si older_button existe et si un lien 'rel=next' est présent\n",
    "        next_page_link = older_button.find('a', rel='next')['href'] if older_button and older_button.find('a', rel='next') else None\n",
    "\n",
    "        # Sortir de la boucle s'il n'y a pas de lien suivant ou si l'URL ne change pas\n",
    "        if not next_page_link or next_page_link == base_url:\n",
    "            break\n",
    "\n",
    "        # Mettre à jour l'URL pour la prochaine itération\n",
    "        base_url = f\"https://www.femelliste.com{next_page_link}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRADFEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\leopo\\OneDrive - GENES\\PDSS\\PDSSS\\scrapping_websites.ipynb Cellule 6\u001b[0m line \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/leopo/OneDrive%20-%20GENES/PDSS/PDSSS/scrapping_websites.ipynb#X12sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m blog_count \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/leopo/OneDrive%20-%20GENES/PDSS/PDSSS/scrapping_websites.ipynb#X12sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/leopo/OneDrive%20-%20GENES/PDSS/PDSSS/scrapping_websites.ipynb#X12sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m     scrape_page(base_url)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/leopo/OneDrive%20-%20GENES/PDSS/PDSSS/scrapping_websites.ipynb#X12sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m     \u001b[39m# Vérifier la présence du lien \"Billets plus anciens\"\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/leopo/OneDrive%20-%20GENES/PDSS/PDSSS/scrapping_websites.ipynb#X12sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m     older_button \u001b[39m=\u001b[39m BeautifulSoup(requests\u001b[39m.\u001b[39mget(base_url)\u001b[39m.\u001b[39mtext, \u001b[39m'\u001b[39m\u001b[39mhtml.parser\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mfind(\u001b[39m'\u001b[39m\u001b[39mdiv\u001b[39m\u001b[39m'\u001b[39m, class_\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mnav-previous\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32mc:\\Users\\leopo\\OneDrive - GENES\\PDSS\\PDSSS\\scrapping_websites.ipynb Cellule 6\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/leopo/OneDrive%20-%20GENES/PDSS/PDSSS/scrapping_websites.ipynb#X12sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m article_url \u001b[39m=\u001b[39m article\u001b[39m.\u001b[39mfind(\u001b[39m'\u001b[39m\u001b[39ma\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m'\u001b[39m\u001b[39mhref\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/leopo/OneDrive%20-%20GENES/PDSS/PDSSS/scrapping_websites.ipynb#X12sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m image_url \u001b[39m=\u001b[39m article\u001b[39m.\u001b[39mfind(\u001b[39m'\u001b[39m\u001b[39mimg\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m'\u001b[39m\u001b[39msrc\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/leopo/OneDrive%20-%20GENES/PDSS/PDSSS/scrapping_websites.ipynb#X12sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m article_response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39;49mget(article_url)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/leopo/OneDrive%20-%20GENES/PDSS/PDSSS/scrapping_websites.ipynb#X12sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mif\u001b[39;00m article_response\u001b[39m.\u001b[39mstatus_code \u001b[39m==\u001b[39m \u001b[39m200\u001b[39m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/leopo/OneDrive%20-%20GENES/PDSS/PDSSS/scrapping_websites.ipynb#X12sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     article_soup \u001b[39m=\u001b[39m BeautifulSoup(article_response\u001b[39m.\u001b[39mtext, \u001b[39m'\u001b[39m\u001b[39mhtml.parser\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\leopo\\anaconda3\\envs\\ml_base\\lib\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(url, params\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[39m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[39m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[39mreturn\u001b[39;00m request(\u001b[39m\"\u001b[39m\u001b[39mget\u001b[39m\u001b[39m\"\u001b[39m, url, params\u001b[39m=\u001b[39mparams, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\leopo\\anaconda3\\envs\\ml_base\\lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[39m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[39m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[39m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39mrequest(method\u001b[39m=\u001b[39mmethod, url\u001b[39m=\u001b[39murl, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\leopo\\anaconda3\\envs\\ml_base\\lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msend(prep, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\Users\\leopo\\anaconda3\\envs\\ml_base\\lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39msend(request, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[1;32mc:\\Users\\leopo\\anaconda3\\envs\\ml_base\\lib\\site-packages\\requests\\adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    483\u001b[0m     timeout \u001b[39m=\u001b[39m TimeoutSauce(connect\u001b[39m=\u001b[39mtimeout, read\u001b[39m=\u001b[39mtimeout)\n\u001b[0;32m    485\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 486\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[0;32m    487\u001b[0m         method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[0;32m    488\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[0;32m    489\u001b[0m         body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[0;32m    490\u001b[0m         headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[0;32m    491\u001b[0m         redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    492\u001b[0m         assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    493\u001b[0m         preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    494\u001b[0m         decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    495\u001b[0m         retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[0;32m    496\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    497\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[0;32m    498\u001b[0m     )\n\u001b[0;32m    500\u001b[0m \u001b[39mexcept\u001b[39;00m (ProtocolError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m    501\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(err, request\u001b[39m=\u001b[39mrequest)\n",
      "File \u001b[1;32mc:\\Users\\leopo\\anaconda3\\envs\\ml_base\\lib\\site-packages\\urllib3\\connectionpool.py:790\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    787\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    789\u001b[0m \u001b[39m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 790\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_request(\n\u001b[0;32m    791\u001b[0m     conn,\n\u001b[0;32m    792\u001b[0m     method,\n\u001b[0;32m    793\u001b[0m     url,\n\u001b[0;32m    794\u001b[0m     timeout\u001b[39m=\u001b[39mtimeout_obj,\n\u001b[0;32m    795\u001b[0m     body\u001b[39m=\u001b[39mbody,\n\u001b[0;32m    796\u001b[0m     headers\u001b[39m=\u001b[39mheaders,\n\u001b[0;32m    797\u001b[0m     chunked\u001b[39m=\u001b[39mchunked,\n\u001b[0;32m    798\u001b[0m     retries\u001b[39m=\u001b[39mretries,\n\u001b[0;32m    799\u001b[0m     response_conn\u001b[39m=\u001b[39mresponse_conn,\n\u001b[0;32m    800\u001b[0m     preload_content\u001b[39m=\u001b[39mpreload_content,\n\u001b[0;32m    801\u001b[0m     decode_content\u001b[39m=\u001b[39mdecode_content,\n\u001b[0;32m    802\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mresponse_kw,\n\u001b[0;32m    803\u001b[0m )\n\u001b[0;32m    805\u001b[0m \u001b[39m# Everything went great!\u001b[39;00m\n\u001b[0;32m    806\u001b[0m clean_exit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\leopo\\anaconda3\\envs\\ml_base\\lib\\site-packages\\urllib3\\connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    534\u001b[0m \u001b[39m# Receive the response from the server\u001b[39;00m\n\u001b[0;32m    535\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 536\u001b[0m     response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[0;32m    537\u001b[0m \u001b[39mexcept\u001b[39;00m (BaseSSLError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    538\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mread_timeout)\n",
      "File \u001b[1;32mc:\\Users\\leopo\\anaconda3\\envs\\ml_base\\lib\\site-packages\\urllib3\\connection.py:461\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    458\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mresponse\u001b[39;00m \u001b[39mimport\u001b[39;00m HTTPResponse\n\u001b[0;32m    460\u001b[0m \u001b[39m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[1;32m--> 461\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[0;32m    463\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    464\u001b[0m     assert_header_parsing(httplib_response\u001b[39m.\u001b[39mmsg)\n",
      "File \u001b[1;32mc:\\Users\\leopo\\anaconda3\\envs\\ml_base\\lib\\http\\client.py:1375\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1373\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1374\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1375\u001b[0m         response\u001b[39m.\u001b[39;49mbegin()\n\u001b[0;32m   1376\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n\u001b[0;32m   1377\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\leopo\\anaconda3\\envs\\ml_base\\lib\\http\\client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[39m# read until we get a non-100 response\u001b[39;00m\n\u001b[0;32m    317\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> 318\u001b[0m     version, status, reason \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_status()\n\u001b[0;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m!=\u001b[39m CONTINUE:\n\u001b[0;32m    320\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\leopo\\anaconda3\\envs\\ml_base\\lib\\http\\client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_read_status\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 279\u001b[0m     line \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadline(_MAXLINE \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m), \u001b[39m\"\u001b[39m\u001b[39miso-8859-1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    280\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(line) \u001b[39m>\u001b[39m _MAXLINE:\n\u001b[0;32m    281\u001b[0m         \u001b[39mraise\u001b[39;00m LineTooLong(\u001b[39m\"\u001b[39m\u001b[39mstatus line\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\leopo\\anaconda3\\envs\\ml_base\\lib\\socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    703\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m    704\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 705\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[0;32m    706\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[0;32m    707\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\leopo\\anaconda3\\envs\\ml_base\\lib\\ssl.py:1274\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1270\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1271\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1272\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[0;32m   1273\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[1;32m-> 1274\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[0;32m   1275\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1276\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mc:\\Users\\leopo\\anaconda3\\envs\\ml_base\\lib\\ssl.py:1130\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1128\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1129\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1130\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[0;32m   1131\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1132\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import os\n",
    "\n",
    "# Fonction pour récupérer les informations d'une page donnée\n",
    "def scrape_page(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        articles = soup.find_all('div', class_='entry-thumbnail')\n",
    "\n",
    "        for article in articles:\n",
    "            article_url = article.find('a')['href']\n",
    "            image_url = article.find('img')['src']\n",
    "\n",
    "            article_response = requests.get(article_url)\n",
    "            if article_response.status_code == 200:\n",
    "                article_soup = BeautifulSoup(article_response.text, 'html.parser')\n",
    "\n",
    "                title_element = article_soup.find('h1', class_='entry-title')\n",
    "                title = title_element.text.strip() if title_element else \"Titre non disponible\"\n",
    "\n",
    "                # Ajout pour récupérer la date\n",
    "                date_element = article_soup.find('a', class_='entry-date')\n",
    "                date = date_element.text.strip() if date_element else \"Date non disponible\"\n",
    "\n",
    "                # Ajout pour récupérer l'auteur\n",
    "                author_element = article_soup.find('a', class_='author')\n",
    "                author = author_element.text.strip() if author_element else \"Auteur non disponible\"\n",
    "\n",
    "                main_content = article_soup.find('div', {'id': 'content', 'class': 'site-content', 'role': 'main'})\n",
    "                full_text = main_content.get_text(strip=True)\n",
    "            else:\n",
    "                full_text = \"Échec de la requête pour l'article.\"\n",
    "\n",
    "            writer.writerow({\n",
    "                'Titre': title,\n",
    "                'Date': date,\n",
    "                'Auteur': author,\n",
    "                'Image URL': image_url,\n",
    "                'Texte Complet': full_text,\n",
    "                'URL Complète': article_url\n",
    "            })\n",
    "\n",
    "            # Incrémenter le compteur\n",
    "            global blog_count\n",
    "            blog_count += 1\n",
    "\n",
    "    else:\n",
    "        print(f\"Échec de la requête avec le code d'état: {response.status_code}\")\n",
    "\n",
    "# Ouvrir un fichier TSV en mode écriture avec un délimiteur tab dans le répertoire \"./text/\"\n",
    "output_file_path = './text/tradfem_articles.tsv'\n",
    "with open(output_file_path, 'w', newline='', encoding='utf-8') as tsvfile:\n",
    "    fieldnames = ['Titre', 'Date', 'Auteur', 'Image URL', 'Texte Complet', 'URL Complète']\n",
    "    writer = csv.DictWriter(tsvfile, fieldnames=fieldnames, delimiter='\\t')\n",
    "    writer.writeheader()\n",
    "\n",
    "    base_url = \"https://tradfem.wordpress.com/\"\n",
    "    blog_count = 0\n",
    "\n",
    "    while True:\n",
    "        scrape_page(base_url)\n",
    "\n",
    "        # Vérifier la présence du lien \"Billets plus anciens\"\n",
    "        older_button = BeautifulSoup(requests.get(base_url).text, 'html.parser').find('div', class_='nav-previous')\n",
    "        next_page_link = older_button.find('a')['href'] if older_button and older_button.find('a') else None\n",
    "\n",
    "        # Sortir de la boucle s'il n'y a pas de lien suivant\n",
    "        if not next_page_link:\n",
    "            break\n",
    "\n",
    "        # Mettre à jour l'URL pour la prochaine itération\n",
    "        base_url = next_page_link\n",
    "\n",
    "# Print le nombre de blogs scannés\n",
    "print(f\"Nombre de blogs scannés : {blog_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Egalité et réconciliation\n",
    "echec car la plus part des liens ne sont pas des postes avec du texte mais renvoie vers autres choses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped page 1 with 10 links.\n",
      "Scraped page 2 with 10 links.\n",
      "Scraped page 3 with 10 links.\n",
      "Scraped page 4 with 10 links.\n",
      "Scraped page 5 with 10 links.\n",
      "Scraped page 6 with 10 links.\n",
      "Scraped page 7 with 10 links.\n",
      "Scraped page 8 with 10 links.\n",
      "Scraped page 9 with 10 links.\n",
      "Scraped page 10 with 10 links.\n",
      "Scraped page 11 with 10 links.\n",
      "Scraped page 12 with 10 links.\n",
      "Scraped page 13 with 10 links.\n",
      "Scraped page 14 with 10 links.\n",
      "Scraped page 15 with 10 links.\n",
      "Scraped page 16 with 10 links.\n",
      "Scraped page 17 with 10 links.\n",
      "Scraped page 18 with 10 links.\n",
      "Scraped page 19 with 10 links.\n",
      "Scraped page 20 with 10 links.\n",
      "Scraped page 21 with 10 links.\n",
      "Scraped page 22 with 10 links.\n",
      "Scraped page 23 with 10 links.\n",
      "Scraped page 24 with 10 links.\n",
      "Scraped page 25 with 10 links.\n",
      "Scraped page 26 with 10 links.\n",
      "Scraped page 27 with 10 links.\n",
      "Scraped page 28 with 10 links.\n",
      "Scraped page 29 with 10 links.\n",
      "Scraped page 30 with 10 links.\n",
      "Scraped page 31 with 10 links.\n",
      "Scraped page 32 with 10 links.\n",
      "Scraped page 33 with 10 links.\n",
      "Scraped page 34 with 10 links.\n",
      "Scraped page 35 with 10 links.\n",
      "Scraped page 36 with 10 links.\n",
      "Scraped page 37 with 10 links.\n",
      "Scraped page 38 with 10 links.\n",
      "Scraped page 39 with 10 links.\n",
      "Scraped page 40 with 10 links.\n",
      "Scraped page 41 with 10 links.\n",
      "Scraped page 42 with 10 links.\n",
      "Scraped page 43 with 10 links.\n",
      "Scraped page 44 with 10 links.\n",
      "Scraped page 45 with 10 links.\n",
      "Scraped page 46 with 10 links.\n",
      "Scraped page 47 with 10 links.\n",
      "Scraped page 48 with 10 links.\n",
      "Scraped page 49 with 10 links.\n",
      "Scraped page 50 with 10 links.\n",
      "Scraped page 51 with 10 links.\n",
      "Scraped page 52 with 10 links.\n",
      "Scraped page 53 with 10 links.\n",
      "Scraped page 54 with 10 links.\n",
      "Scraped page 55 with 10 links.\n",
      "Scraped page 56 with 10 links.\n",
      "Scraped page 57 with 10 links.\n",
      "Scraped page 58 with 10 links.\n",
      "Scraped page 59 with 10 links.\n",
      "Scraped page 60 with 10 links.\n",
      "Scraped page 61 with 10 links.\n",
      "Scraped page 62 with 10 links.\n",
      "Scraped page 63 with 10 links.\n",
      "Scraped page 64 with 10 links.\n",
      "Scraped page 65 with 10 links.\n",
      "Scraped page 66 with 10 links.\n",
      "Scraped page 67 with 10 links.\n",
      "Scraped page 68 with 10 links.\n",
      "Scraped page 69 with 10 links.\n",
      "Scraped page 70 with 10 links.\n",
      "Scraped page 71 with 10 links.\n",
      "Scraped page 72 with 10 links.\n",
      "Scraped page 73 with 10 links.\n",
      "Scraped page 74 with 10 links.\n",
      "Scraped page 75 with 10 links.\n",
      "Scraped page 76 with 10 links.\n",
      "Scraped page 77 with 10 links.\n",
      "Scraped page 78 with 10 links.\n",
      "Scraped page 79 with 10 links.\n",
      "Scraped page 80 with 10 links.\n",
      "Scraped page 81 with 10 links.\n",
      "Scraped page 82 with 10 links.\n",
      "Scraped page 83 with 10 links.\n",
      "Scraped page 84 with 10 links.\n",
      "Scraped page 85 with 10 links.\n",
      "Scraped page 86 with 10 links.\n",
      "Scraped page 87 with 10 links.\n",
      "Total links collected: 870\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Base URL of the website\n",
    "base_url = 'https://www.egaliteetreconciliation.fr/'\n",
    "\n",
    "# Function to scrape a single page\n",
    "def scrape_page(url):\n",
    "    # Use requests to fetch the content of the website\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # This will raise an exception for HTTP error codes\n",
    "\n",
    "    # Use BeautifulSoup to parse the HTML content\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all <a> tags within <h4> tags\n",
    "    post_links = soup.find_all('h4')\n",
    "    links = [base_url + a.find('a')['href'] for a in post_links if a.find('a')]\n",
    "\n",
    "    return links\n",
    "\n",
    "# Initialize an empty list to hold all links\n",
    "all_links = []\n",
    "\n",
    "# Loop through all pages\n",
    "for page in range(1, 88):  # 87 pages in total\n",
    "    if page == 1:\n",
    "        # The URL for the first page\n",
    "        page_url = base_url + 'spip.php?page=alaune'\n",
    "    else:\n",
    "        # Calculate the debut_artg value for the current page\n",
    "        debut_artg = (page - 1) * 10\n",
    "        page_url = base_url + f'spip.php?page=alaune&debut_artg={debut_artg}#pagination_artg'\n",
    "    \n",
    "    # Scrape the current page\n",
    "    links = scrape_page(page_url)\n",
    "    all_links.extend(links)\n",
    "    print(f\"Scraped page {page} with {len(links)} links.\")\n",
    "\n",
    "# Optionally, print all collected links (commented out to avoid flooding the output)\n",
    "# for link in all_links:\n",
    "#     print(link)\n",
    "\n",
    "print(f\"Total links collected: {len(all_links)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Title            Author      Publication Date  \\\n",
      "0  Title not found  Author not found  2024-01-23T09:48:56Z   \n",
      "1  Title not found  Author not found  2024-01-15T09:09:34Z   \n",
      "2  Title not found  Author not found  2024-01-12T19:18:54Z   \n",
      "3  Title not found  Author not found  2023-12-29T13:32:20Z   \n",
      "4  Title not found  Author not found  2023-12-26T22:55:00Z   \n",
      "\n",
      "                                                 URL            Text  \n",
      "0  https://www.egaliteetreconciliation.fr/C-est-p...  Text not found  \n",
      "1  https://www.egaliteetreconciliation.fr/Soral-e...  Text not found  \n",
      "2  https://www.egaliteetreconciliation.fr/Pierre-...  Text not found  \n",
      "3  https://www.egaliteetreconciliation.fr/Faits-D...  Text not found  \n",
      "4  https://www.egaliteetreconciliation.fr/Jean-Mi...  Text not found  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize an empty list to hold dictionaries of the scraped data\n",
    "scraped_data = []\n",
    "\n",
    "# Loop through each URL in the all_links list\n",
    "for url in all_links:\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Extract the title\n",
    "        title_h1 = soup.find('h1', class_='crayon article-titre')\n",
    "        title = title_h1.text.strip() if title_h1 else 'Title not found'\n",
    "        \n",
    "        # Extract the subtitle (as author in this context)\n",
    "        subtitle_h2 = soup.find('h2', class_='crayon article-soustitre')\n",
    "        author = subtitle_h2.text.strip() if subtitle_h2 else 'Author not found'\n",
    "        \n",
    "        # Extract the publication date\n",
    "        pub_date = soup.find('abbr', class_='published')\n",
    "        publication_date = pub_date['title'] if pub_date else 'Date not found'\n",
    "        \n",
    "        # Extract the main text\n",
    "        main_text_div = soup.find('div', class_='crayon article-texte')\n",
    "        main_text = main_text_div.text.strip() if main_text_div else 'Text not found'\n",
    "        \n",
    "        # Append the extracted information as a dictionary to the list\n",
    "        scraped_data.append({\n",
    "            'Title': title,\n",
    "            'Author': author,\n",
    "            'Publication Date': publication_date,\n",
    "            'URL': url,\n",
    "            'Text': main_text\n",
    "        })\n",
    "\n",
    "# Convert the list of dictionaries into a pandas DataFrame\n",
    "df = pd.DataFrame(scraped_data)\n",
    "\n",
    "# Display the first few rows of the DataFrame to verify\n",
    "print(df.head())\n",
    "\n",
    "# Optionally, save the DataFrame to a CSV file\n",
    "df.to_csv('../data/text/EetR_articles.tsv', sep='\\t', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# La petite sirène"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "base_url = \"https://www.observatoirepetitesirene.org\"\n",
    "articles_list_url = \"https://www.observatoirepetitesirene.org/nospapiers\"\n",
    "\n",
    "# Send a GET request\n",
    "response = requests.get(articles_list_url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Assuming articles are linked within <a> tags directly within a specific class or id\n",
    "# You'll need to inspect the HTML structure of the page to adjust the selector accordingly\n",
    "article_links = []\n",
    "for link in soup.select(\"selector-for-article-links\"):  # Update this selector based on actual HTML\n",
    "    href = link.get('href')\n",
    "    if href and not href.startswith('http'):\n",
    "        href = base_url + href  # Construct full URL if needed\n",
    "    article_links.append(href)\n",
    "\n",
    "print(article_links)\n",
    "\n",
    "data = []  # Initialize an empty list to store article data\n",
    "\n",
    "for url in article_links:\n",
    "    response = requests.get(url)\n",
    "    article_soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Extract title - Adjust the selector based on the actual HTML structure\n",
    "    title = article_soup.find('tag-for-title', class_='class-for-title').text.strip()\n",
    "    \n",
    "    # Extract publication date - Adjust the selector based on the actual HTML structure\n",
    "    pub_date = article_soup.find('tag-for-date', class_='class-for-date')['attribute-for-date']\n",
    "    \n",
    "    # Adjust author extraction based on the actual HTML structure or set statically if constant\n",
    "    author = 'Default Author'  # Static example, adjust as needed\n",
    "    \n",
    "    # Extract the text - Adjust the selector based on the actual HTML structure\n",
    "    text_parts = article_soup.select('selector-for-article-text p')\n",
    "    text = ' '.join(part.text for part in text_parts)\n",
    "    \n",
    "    # Append the information as a dictionary to the data list\n",
    "    data.append({\n",
    "        'Title': title,\n",
    "        'Publication Date': pub_date,\n",
    "        'Author': author,\n",
    "        'URL': url,\n",
    "        'Text': text\n",
    "    })\n",
    "\n",
    "# Example to demonstrate data structure, remove or adjust as needed for actual use\n",
    "for article in data[:1]:  # Print first article's data as an example\n",
    "    print(article)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Christine Le Doaré"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the summary page listing all articles\n",
    "summary_url = 'https://christineld75.wordpress.com/sommaire-des-articles/'\n",
    "\n",
    "# Send a request to the summary page\n",
    "response = requests.get(summary_url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Extract all article links from the summary page\n",
    "article_links = [a['href'] for a in soup.select('a[href]') if 'christineld75.wordpress.com' in a['href']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []  # Initialize an empty list to store article data\n",
    "\n",
    "for url in article_links:\n",
    "    response = requests.get(url)\n",
    "    article_soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Extract title with error handling\n",
    "    title_element = article_soup.find('h3', class_='entry-title')\n",
    "    title = title_element.text.strip() if title_element else 'Title Not Found'\n",
    "    \n",
    "    # Extract publication date with error handling\n",
    "    pub_date_element = article_soup.find('abbr', class_='published')\n",
    "    pub_date = pub_date_element['title'] if pub_date_element else 'Date Not Found'\n",
    "    \n",
    "    # Author is always 'Christine Le Doaré'\n",
    "    author = 'Christine Le Doaré'\n",
    "    \n",
    "    # Extract the text\n",
    "    text_parts = article_soup.select('div.entry-content p')\n",
    "    text = ' '.join(part.text for part in text_parts) if text_parts else 'Text Not Found'\n",
    "    \n",
    "    # Append the information as a dictionary to the data list\n",
    "    data.append({\n",
    "        'Title': title,\n",
    "        'Publication Date': pub_date,\n",
    "        'Author': author,\n",
    "        'URL': url,\n",
    "        'Text': text\n",
    "    })\n",
    "\n",
    "\n",
    "# Convert the list of dictionaries into a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save the DataFrame to a TSV file\n",
    "df.to_csv('../data/text/christineld_articles.tsv', sep='\\t', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observatoire de la petite sirène"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.observatoirepetitesirene.org/post/papier-de-jacqueline-schaeffer-n-4\n",
      "https://www.observatoirepetitesirene.org/post/papier-de-christian-flavigny-n-3\n",
      "https://www.observatoirepetitesirene.org/post/papier-de-françois-rastier-n-2\n",
      "https://www.observatoirepetitesirene.org/post/papier-de-claudio-rubiliani-n-1\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "base_url = \"https://www.observatoirepetitesirene.org\"\n",
    "articles_list_url = \"https://www.observatoirepetitesirene.org/nospapiers\"\n",
    "\n",
    "# Send a request to the summary page\n",
    "response = requests.get(articles_list_url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Continuation du code\n",
    "article_links = []\n",
    "\n",
    "# Supposons que les liens soient enveloppés dans un élément `a` près du titre de l'article.\n",
    "# Vous devrez adapter le sélecteur CSS ci-dessous selon la structure réelle de la page.\n",
    "for block in soup.select(\".FbwBsX\"):  # Utilisez la classe du bloc pour localiser les titres\n",
    "    # Recherche de l'élément `a` le plus proche ou à l'intérieur du bloc sélectionné\n",
    "    # Ceci est un exemple générique; vous devrez peut-être ajuster le sélecteur\n",
    "    link = block.find_parent('a')\n",
    "    if link:\n",
    "        article_url = link.get('href')\n",
    "        # Assurez-vous que l'URL est complète; sinon, concaténez avec base_url\n",
    "        if article_url and not article_url.startswith('http'):\n",
    "            article_url = base_url + article_url\n",
    "        article_links.append(article_url)\n",
    "\n",
    "# Affichez ou traitez les liens des articles récupérés\n",
    "for url in article_links:\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "# Initialize a list to store scraped data\n",
    "articles_data = []\n",
    "\n",
    "# Function to scrape an individual article\n",
    "def scrape_article(article_url):\n",
    "    response = requests.get(article_url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # You would adjust the selectors below based on the actual HTML structure of your target pages\n",
    "    title = soup.find('h1').text.strip()  # Example: Adjust selector as needed\n",
    "    # Extract author\n",
    "    author_block = soup.find('span', class_='tQ0Q1A user-name dlINDG')\n",
    "    author = author_block.text if author_block else 'Unknown'\n",
    "    \n",
    "    # Extract publication date\n",
    "    date_block = soup.find('span', class_='post-metadata__date time-ago')\n",
    "    if date_block:\n",
    "        # Parse the date assuming the format is like \"30 janv.\"\n",
    "        pub_date_str = date_block.text # + \" 2024\"  # Assuming year is 2024\n",
    "        # Convert French month name to number and format the date\n",
    "        pub_date = pub_date_str # datetime.strptime(pub_date_str, '%d %m %Y').strftime('%d/%m/%Y')\n",
    "    else:\n",
    "        pub_date = 'Unknown'\n",
    "\n",
    "\n",
    "    # Initialize a variable to store the concatenated text\n",
    "    article_text = \"\"\n",
    "    \n",
    "    # Iterate over blocks to extract text (this part needs customization based on the actual HTML structure)\n",
    "    for block in soup.select('div[data-breakout=\"normal\"]'):\n",
    "        # Extract text or handle specific types of content here\n",
    "        text_content = block.get_text(strip=True)\n",
    "        article_text += text_content + \" \"    \n",
    "    return {\n",
    "        'title': title,\n",
    "        'url':article_url,\n",
    "        'publication_date': pub_date,\n",
    "        'author': author,\n",
    "        'text': article_text,\n",
    "    }\n",
    "\n",
    "# Iterate over article links and scrape each article\n",
    "for url in article_links:\n",
    "    article_data = scrape_article(url)\n",
    "    articles_data.append(article_data)\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame for easier manipulation and saving\n",
    "df = pd.DataFrame(articles_data)\n",
    "\n",
    "# Optionally, save the DataFrame to a CSV file\n",
    "df.to_csv('../data/text/petitesirene_articles.tsv', sep='\\t', index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
