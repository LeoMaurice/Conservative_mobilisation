{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "22f957dd-0b41-4614-a208-c48f9a960f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wordcloud in /opt/mamba/lib/python3.10/site-packages (1.9.3)\n",
      "Requirement already satisfied: numpy>=1.6.1 in /opt/mamba/lib/python3.10/site-packages (from wordcloud) (1.26.3)\n",
      "Requirement already satisfied: pillow in /opt/mamba/lib/python3.10/site-packages (from wordcloud) (10.2.0)\n",
      "Requirement already satisfied: matplotlib in /opt/mamba/lib/python3.10/site-packages (from wordcloud) (3.8.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/mamba/lib/python3.10/site-packages (from matplotlib->wordcloud) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/mamba/lib/python3.10/site-packages (from matplotlib->wordcloud) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/mamba/lib/python3.10/site-packages (from matplotlib->wordcloud) (4.47.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/mamba/lib/python3.10/site-packages (from matplotlib->wordcloud) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/mamba/lib/python3.10/site-packages (from matplotlib->wordcloud) (23.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/mamba/lib/python3.10/site-packages (from matplotlib->wordcloud) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/mamba/lib/python3.10/site-packages (from matplotlib->wordcloud) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/mamba/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n",
      "Requirement already satisfied: nltk in /opt/mamba/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /opt/mamba/lib/python3.10/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/mamba/lib/python3.10/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/mamba/lib/python3.10/site-packages (from nltk) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in /opt/mamba/lib/python3.10/site-packages (from nltk) (4.66.1)\n",
      "Collecting pattern\n",
      "  Using cached Pattern-3.6.0.tar.gz (22.2 MB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting future (from pattern)\n",
      "  Using cached future-0.18.3.tar.gz (840 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting backports.csv (from pattern)\n",
      "  Using cached backports.csv-1.0.7-py2.py3-none-any.whl (12 kB)\n",
      "Collecting mysqlclient (from pattern)\n",
      "  Using cached mysqlclient-2.2.1.tar.gz (89 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[27 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m /bin/sh: 1: pkg-config: not found\n",
      "  \u001b[31m   \u001b[0m /bin/sh: 1: pkg-config: not found\n",
      "  \u001b[31m   \u001b[0m /bin/sh: 1: pkg-config: not found\n",
      "  \u001b[31m   \u001b[0m Trying pkg-config --exists mysqlclient\n",
      "  \u001b[31m   \u001b[0m Command 'pkg-config --exists mysqlclient' returned non-zero exit status 127.\n",
      "  \u001b[31m   \u001b[0m Trying pkg-config --exists mariadb\n",
      "  \u001b[31m   \u001b[0m Command 'pkg-config --exists mariadb' returned non-zero exit status 127.\n",
      "  \u001b[31m   \u001b[0m Trying pkg-config --exists libmariadb\n",
      "  \u001b[31m   \u001b[0m Command 'pkg-config --exists libmariadb' returned non-zero exit status 127.\n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/mamba/lib/python3.10/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 353, in <module>\n",
      "  \u001b[31m   \u001b[0m     main()\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/mamba/lib/python3.10/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 335, in main\n",
      "  \u001b[31m   \u001b[0m     json_out['return_val'] = hook(**hook_input['kwargs'])\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/mamba/lib/python3.10/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 118, in get_requires_for_build_wheel\n",
      "  \u001b[31m   \u001b[0m     return hook(config_settings)\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-g6h41jr1/overlay/lib/python3.10/site-packages/setuptools/build_meta.py\", line 325, in get_requires_for_build_wheel\n",
      "  \u001b[31m   \u001b[0m     return self._get_build_requires(config_settings, requirements=['wheel'])\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-g6h41jr1/overlay/lib/python3.10/site-packages/setuptools/build_meta.py\", line 295, in _get_build_requires\n",
      "  \u001b[31m   \u001b[0m     self.run_setup()\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-g6h41jr1/overlay/lib/python3.10/site-packages/setuptools/build_meta.py\", line 311, in run_setup\n",
      "  \u001b[31m   \u001b[0m     exec(code, locals())\n",
      "  \u001b[31m   \u001b[0m   File \"<string>\", line 155, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"<string>\", line 49, in get_config_posix\n",
      "  \u001b[31m   \u001b[0m   File \"<string>\", line 28, in find_package_name\n",
      "  \u001b[31m   \u001b[0m Exception: Can not find valid pkg-config name.\n",
      "  \u001b[31m   \u001b[0m Specify MYSQLCLIENT_CFLAGS and MYSQLCLIENT_LDFLAGS env vars manually\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
      "\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/onyxia/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/onyxia/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/onyxia/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#On va importer les packages \n",
    "!pip install wordcloud \n",
    "!pip install nltk\n",
    "!pip install pattern\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('french'))\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "6913801a-e65d-4054-bb1d-a5a9b9d657f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Récupérations données \n",
    "base_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "# Construisez le chemin complet vers le répertoire de données\n",
    "fichier = os.path.join(base_dir, 'data', 'base_def.csv')\n",
    "\n",
    "df = pd.read_csv(fichier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddf0a0c-6c0d-4de2-82da-ea3ab72156fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Travail des stop words\n",
    "mots_fr = [\n",
    "    'le', 'la', 'les', 'de', 'un', 'à', 'être', 'suis', 'es', 'est', 'sont', 'été', 'étais', 'était', 'étaient', 'serai', 'seras', 'sera', 'seront',\n",
    "    'et', 'en', 'avoir', 'ai', 'as', 'a', 'ont', 'eu', 'aurai', 'auras', 'aura', 'auront', 'avais', 'avait', 'avaient', 'que', 'pour',\n",
    "    'dans', 'ce', 'il', 'qui', 'ne', 'en', 'sur', 'se', 'pas', 'plus',\n",
    "    'par', 'je', 'avec', 'tout', 'faire', 'fais', 'fait', 'font', 'son', 'mettre', 'autre', 'on', 'mais',\n",
    "    'nous', 'comme', 'ou', 'si', 'leur', 'y', 'dire', 'dis', 'dit', 'disent' 'elle', 'voir', 'devoir',\n",
    "    'deux', 'même', 'pendant', 'aussi', 'vouloir', 'grand', 'mon', 'nouveau', 'aller',\n",
    "    'venir', 'ceux', 'faire', 'quelque', 'trouver', 'donner', 'donne', 'donnes', 'donnent', 'aussi', 'autre',\n",
    "    'tous', 'vieux', 'bon', 'voir', 'moins', 'trois', 'avant', 'sa', 'faire', 'contre',\n",
    "    'abord', 'sous', 'ou', 'apporter', 'grand', 'ainsi', 'long', 'très', 'tout', 'avoir',\n",
    "    'beau', 'chaque', 'peu', 'quoi', 'encore', 'aller', 'montrer', 'semaine', 'ainsi',\n",
    "    'nuit', 'aussi', 'bien', 'deuxième', 'moins', 'tout', 'avoir', 'peu', 'nuit', 'ça',\n",
    "    'mon', 'ma', 'mes', 'ton', 'ta', 'tes', 'son', 'sa', 'ses', 'notre', 'notre', 'nos', 'votre', 'votre', 'vos', 'leur', 'leur', 'leurs',\n",
    "    'non', '’', '.', ',', ':', 'est', 'des', \"l'\", \"d'\", \"une\", \"d\", \"»\", \"«\", \"du\", \"qu\", \"au\", \"n\", \"s\", \"elle\", \")\", \"(\", \"aux\", 'c', 'cette', 'un', 'ces','?', 'il', 'ils', 'j', \"l\", \"-\", \"vous\", \"entre\",\n",
    "    \"[\", \"]\", \"elles\", '”', '“', \"dont\", \"cela\", \";\", \"m\", \"selon\", \"!\", '–'\n",
    "]\n",
    "# Concaténer tous les textes de la colonne 'contenu' en une seule chaîne\n",
    "all_text = ' '.join(df['contenu'])\n",
    "\n",
    "# Fonction pour enlever les mots de la liste mots_fr\n",
    "def remove_words(text):\n",
    "    words = word_tokenize(text.lower())  # normaliser la casse ici\n",
    "    filtered_words = [word for word in words if word not in mots_fr]\n",
    "    return ' '.join(filtered_words)\n",
    "    \n",
    "# Appliquer la fonction remove_words\n",
    "df['contenu'] = df['contenu'].apply(lambda x: remove_words(x))\n",
    "\n",
    "# Fonction pour normaliser les mots au singulier [va falloir singulariser autrement]\n",
    "def singularize_word(word):\n",
    "    # Retire le 's' à la fin du mot si présent et si la version sans 's' existe déjà strictement\n",
    "    if word.lower().endswith('s') and word[:-1].lower() in set(all_text.split()):\n",
    "        return word[:-1]\n",
    "    else:\n",
    "        return word\n",
    "        \n",
    "df['contenu'] = df['contenu'].apply(lambda x: ' '.join([singularize_word(word) for word in x.split()]))\n",
    "\n",
    "all_text2 = ' '.join(df['contenu'])\n",
    "\n",
    "# Fonction pour obtenir les mots les plus représentés\n",
    "def get_most_common_words(text):\n",
    "    words = word_tokenize(text.lower())\n",
    "    freq_dist = FreqDist(words)\n",
    "    return freq_dist.most_common()\n",
    "\n",
    "# Appliquer la fonction à l'ensemble du texte\n",
    "most_common_words = get_most_common_words(all_text2)\n",
    "most_common_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce395f1-9791-4599-b73e-cf8a81f66c96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
