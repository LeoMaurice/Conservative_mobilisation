---
title: "analyse_lexico"
format: html
editor: source
---

```{r}
if (!require("pacman")) install.packages("pacman"); library(pacman)
pacman::p_load(tidyverse,
               lubridate,
               here,
               knitr,
               
               quanteda,
               quanteda.textstats,
               stm,
               topicmodels,
               LDAvis,
               ldatuning,
               quanteda.textplots,
               
               ggprism,
               ggpmisc,
               RJSONIO,
               ggrepel,
               ggcorrplot,
               ggpubr,
               
               stringr,
               readr,
               readxl
            )

couleurs_ideologie <- ggprism::prism_colour_pal("colorblind_safe")(6)[-1]

# Some little helps from the internet
source("../src/helpers/lda_reports.R")

save_figure = FALSE

seed = 24021956 # date de naissance de Butler
set.seed(seed)
```

# Ouverture données
```{r}
Base_de_données_anti_trans <- read_excel("../data/Base de données anti trans.xlsx", 
    col_types = c("text", "text", "skip", 
        "skip", "text", "skip", "skip", "skip"))|>
  rename(ID_ASSO = Id,
         Asso = nom,
         Idéologie = Idéologie_simplifiée)|>
  mutate(Idéologie = as.factor(Idéologie))
```

```{r}
base_articles <- read_delim("../data/intermediate/base_lemmatized.csv", 
    delim = ";", escape_double = FALSE, col_types = cols(`ID ASSO` = col_character(), 
        `ID ARTICLE` = col_character(), contenu = col_character(), 
        `Type de document` = col_character(), 
        Auteur = col_character(), Date = col_datetime(format = "%d/%m/%Y"), 
        Titre = col_character(), URL = col_character(), 
        lemmatized_contenu = col_character()), 
    trim_ws = TRUE)|>
  select(-URL)|>
  rename(ID = "ID ARTICLE",
         ID_ASSO = "ID ASSO",
         text = "lemmatized_contenu",
         Type_doc = "Type de document")|>
  left_join(Base_de_données_anti_trans, by = "ID_ASSO")

```


```{r}
base_articles$text = base_articles$text|>
  str_replace_all("\\s+", " ") |>
  str_replace_all("[']", " ") |>
  str_replace_all("[’]", " ") |>
  str_replace_all("[`]", " ") |>
  str_replace_all("\r", " ") |>
  str_replace_all("\n", " ") |>
  str_replace_all("[,]",".")
```

```{r}
base_articles$Asso|> table()
```


```{r}
base_articles|>
  group_by(ID_ASSO)|>
  slice_sample(n = 1)|>
  ungroup()|>View()

```


```{r}
keywords <- c("trans", "transexuel", "transexuelle", "transexuels", "transexuelles",
              "transgenre", "transgenres", "transidentité", "transsexualité", "transsexualités",
              "transgender", "non-binaire", "non-binaires", "genderqueer", "intersexe", "intersexes",
              "queer", "affirmant le genre", "affirmant les genres", "transition de genre",
              "transitions de genre", "identité de genre", "identités de genre")

# Construction d'une expression régulière à partir des mots-clés pour la recherche
regex_keywords <- paste(keywords, collapse = "|")

# Filtrage des lignes du data.frame basé sur la présence d'au moins un mot-clé dans 'text'
filtered_data <- base_articles |>
  filter(grepl(regex_keywords, text, ignore.case = TRUE))

cp <- corpus(filtered_data$text, 
             docvars = filtered_data |> select(Asso, Date, Type_doc, Idéologie) |> as.data.frame(), 
             docnames = filtered_data$ID)

# words to remove
toremove <- readLines("../data/intermediate/words_to_filter.txt")
toremove <- c(stopwords("en"),stopwords("fr"),toremove)

# tokenisation
tk <- tokens(cp, remove_punct = TRUE, remove_numbers = TRUE)
tk <- tokens_remove(tk,toremove)
tk <- tokens_replace(tk, pattern = "tran", replacement = "trans", valuetype = "fixed") # effect of spacy tokenization

dfm <- dfm(tk) |>
  dfm_remove(toremove) |>
  dfm_trim(min_termfreq = 5)
```

```{r}
textstat_frequency(dfm, n=100)
```


# Stat desc 

## tableau d'occurences des associations et des idéologies

```{r}
# Calculer les nombres d'occurence
occurence_asso <- table(filtered_data$Asso) |> 
  as.data.frame() |>
  rename(Association = Var1, Occurences = Freq) |>
  arrange(desc(Occurences))

latex_table <- kable(occurence_asso, format = "latex", caption = "Occurences des Associations")

# Sauvegarder le tableau LaTeX dans un fichier
if(save_figure){
  writeLines(latex_table, "../output/descriptive/tableau_occurence_asso.tex")
}

kable(occurence_asso, format = "html", caption = "Occurences des Associations")
```

```{r}
# Calculer les nombres d'occurence
occurence_asso <- table(filtered_data$Idéologie) |> 
  as.data.frame() |>
  rename(Idéologie = Var1, Occurences = Freq) |>
  arrange(desc(Occurences))

latex_table <- kable(occurence_asso, format = "latex", caption = "Occurences des idéologies")

# Sauvegarder le tableau LaTeX dans un fichier
if(save_figure){
  writeLines(latex_table, "../output/descriptive/tableau_occurence_idéo.tex")
}

kable(occurence_asso, format = "html", caption = "Occurences des Associations")
```

```{r}
hist_occurences_doc <- ggplot(occurence_asso, aes(x = Idéologie, y = Occurences, fill = Idéologie)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "Nombre de documents par idéologie",
       x = "Idéologie", y = "Occurences") +
  theme_prism() +
  scale_fill_manual(values = couleurs_ideologie) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1))

if(save_figure){
  ggsave("../output/descriptive/hist_occurences_doc_ideologies.pdf", plot = hist_occurences_doc)
}

hist_occurences_doc

```

## Frequency

```{r}
dfm |> 
  textstat_frequency(n = 15) |> 
  ggplot(aes(x = reorder(feature, frequency), y = frequency)) +
  geom_point() +
  coord_flip() +
  labs(x = NULL, y = "Occurences") +
  theme_minimal()
```


```{r}
#| eval: true
#| output: true
#| fig-cap: Mots les plus fréquents dans le corpus. La taille est proportionnelle à la fréquence.
#| label: fig-wordcloud
palette <-ggprism::prism_color_pal("winter_bright")(9)
textplot_wordcloud(dfm, min_count = 500, random_order = FALSE, rotation = 0.25,
                   color = palette)
```

## Fréquence certains mots par idéologie
```{r}
# Liste des mots clés
keywords <- c("pouvoir", "genre", "enfant", "lesbien")

# Filtrer le DFM pour ne garder que les mots clés
dfm_keywords <- dfm_select(dfm, pattern = keywords, selection = "keep")

# Convertir le DFM filtré en dataframe pour l'analyse
df_keywords <- convert(dfm_keywords, to = "data.frame")|>
  rename(ID = doc_id)|>
  left_join(filtered_data|>
              mutate(n_words=nchar(text))|>
              select(ID,n_words,Idéologie), by = "ID")

# # Ajouter la covariable 'type_doc' depuis votre dataframe 'data'
# df_keywords$Idéologie <- filtered_data$Idéologie

# Calculer la moyenne des fréquences pour chaque mot clé par Idéologie
df_freq <- df_keywords |> 
  mutate(across(keywords,~ 1000 * .x / n_words))|>
  group_by(Idéologie) |>
  summarise(across(keywords, mean, na.rm = TRUE)) |>
  pivot_longer(-Idéologie, names_to = "Mot", values_to = "Fréquences")

hist_freq_keywords <- ggplot(df_freq, aes(x = Mot, y = Fréquences, fill = Idéologie)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "Fréquence des mots clés par idéologie",
       x = "Mot clé", y = "Fréquence moyenne par document (‰)") +
  theme_prism() +
  scale_fill_manual(values = couleurs_ideologie) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1))
  

if(save_figure){
  ggsave("../output/descriptive/hist_freq_keywords_ideologies.pdf", plot = hist_freq_keywords)
}

hist_freq_keywords
```
```{r}
# Liste des mots clés
keywords <- c("pouvoir", "genre", "enfant", "lesbien")

# Filtrer le DFM pour ne garder que les mots clés
dfm_keywords <- dfm_select(dfm, pattern = keywords, selection = "keep")

# Convertir le DFM filtré en dataframe pour l'analyse
df_keywords <- convert(dfm_keywords, to = "data.frame")|>
  rename(ID = doc_id)|>
  left_join(filtered_data|>
              mutate(n_words=nchar(text))|>
              select(ID,n_words,Idéologie), by = "ID")

# Calculer la moyenne des fréquences pour chaque mot clé par Idéologie
df_freq <- df_keywords |> 
  mutate(across(keywords,~ 1000 * .x / n_words))|>
  select(any_of(keywords),Idéologie)|>
  pivot_longer(-Idéologie, names_to = "Mot", values_to = "Fréquences")

boxplot_freq_keywords <- ggboxplot(data=df_freq, x = "Mot", y = "Fréquences", facet.by = "Idéologie", fill = "Idéologie") +
  labs(title = "Fréquence des mots clés par idéologie",
       x = "Mot clé", y = "Fréquence moyenne par document (‰)") +
  theme_prism() +
  scale_fill_manual(values = couleurs_ideologie) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1))
  

if(save_figure){
  ggsave("../output/descriptive/boxplot_freq_keywords.pdf", plot = boxplot_freq_keywords)
}

boxplot_freq_keywords
```


# Choix nombre de topics

```{r}
#| eval: true
recompute_choix_K = FALSE
tm_data <- quanteda::convert(dfm, to = "topicmodels")
stm_data <- convert(dfm, to = "stm")


if(recompute_choix_K){
  list_nb_topics_K <- seq(5, 20, 1)

  tp_nb <- FindTopicsNumber(tm_data, topics = list_nb_topics_K, 
                          metrics = c("Griffiths2004", "CaoJuan2009", 
                                      "Arun2010", "Deveaud2014"),
                          method = "Gibbs")

  diag <- searchK(stm_data$documents, stm_data$vocab, 
                  list_nb_topics_K, 
                  verbose=FALSE)
  save(tp_nb,list_nb_topics_K,diag, file = "../data/results/LDA_report_20240222_metrics.RData")
} else{
  load("../data/results/LDA_report_20240222_metrics.RData")
}

```

## Métriques

```{r}
#| eval: true
#| output: true
#| fig-cap: Evolution des différentes métriques en fonction du nombre de topics
#| fig-subcap: Commenter métriques/résultats
#| label: fig-lda-metrics

FindTopicsNumber_plot(tp_nb)

```

```{r}
tp_nb_custom <- tp_nb|>
  pivot_longer(cols=c("Griffiths2004","CaoJuan2009","Arun2010","Deveaud2014"), names_to = "metricsType",values_to = "metrics")|>
  mutate(category = ifelse(metricsType %in% c("Arun2010","CaoJuan2009"),"minimize","maximize"))|>
  group_by(metricsType)|>
  mutate(max = max(metrics),
         min = min(metrics))|>
  mutate(metrics = (metrics-min)/(max-min))|>
  ungroup()
  

custom_tp_nb_plot <- ggplot(tp_nb_custom, aes(x = topics, y = metrics, color = metricsType, group = metricsType, shape = metricsType)) +
  geom_line() +
  geom_point() +
  facet_wrap(~category, ncol = 1) + # Separate plots for Minimize vs Maximize
  scale_color_prism(palette = "winter_bright") +
  theme_prism() + # Use ggpubr theme
  labs(x = "number of topics", y = NULL, color = "metrics:",shape = "metrics:") +
  theme(legend.position = "right",
        legend.title = element_text())
if(save_figure){
  ggsave("../output/lda/find_topic_number_metrics.pdf",plot = custom_tp_nb_plot)
}
custom_tp_nb_plot
```

## Exclusivité et cohérence sémantique
```{r}
#| eval: true
#| output: true
#| fig-cap: Cohérence sémantique et excluvité en fonction du nombre de topics
#| label: fig-lda-exclu_cohsem
map(diag$results, unlist) |> 
  bind_cols() |> 
  ggplot(aes(exclus, semcoh, label = K)) +
    geom_point() +
    geom_label() +
  theme_prism()
```

# STM
```{r}
num_topic = 9
```

```{r}
#| eval: true

stm_data <- convert(dfm, to = "stm")

stm_lda <- stm(dfm,
               K=num_topic, 
               prevalence =~factor(type_doc) + factor(type_asso) ,
               #content =~ ,
               init.type = "Spectral",
               seed = seed, 
               verbose = FALSE)
```

```{r}
toLDAvis(stm_lda, stm_data$documents,reorder.topics = FALSE)

```
```{r}
labelTopics(stm_lda)

```
```{r}
plot(stm_lda, type = "summary", labeltype = "prob")

```
```{r}
plot(stm_lda, type = "labels", labeltype = "prob", frexw=0.5)
```
```{r}
plot(stm_lda, type = "perspectives", topics = c(4,3))

```

```{r}
corrmat <- topicCorr(stm_lda)
plot(corrmat)
```
## Effet des métadonnées

```{r}
#| eval: true
stm_lda.effect <- estimateEffect(formula = 1:num_topic ~ factor(ASSO) + factor(type_doc), stmobj = stm_lda, metadata = stm_data$meta, uncertainty="Global")
```
```{r}
summary_stm_main_effects
```

```{r}
# Supposons que 'effets' est l'objet retourné par estimateEffect
effets <- estimateEffect(formula = 1:num_topic ~ factor(type_asso) + factor(type_doc), 
                         stmobj = stm_lda, metadata = stm_data$meta, uncertainty = "Global")

# Extraction des résumés des effets pour chaque sujet
summaries <- summary(effets, topics = 1:num_topic)

# Pour chaque sujet, filtrer et afficher uniquement les coefficients significatifs

cat("\n\n\n\n")
for (i in 1:num_topic) {
  cat("Topic", i, ":\n\n")
  
  # Extrait les coefficients et p-valeurs pour le sujet courant
  coeffs <- summaries$tables[[i]]
  
  # Filtrer pour garder uniquement les coefficients significatifs
  significant_coeffs <- coeffs[coeffs[, "Pr(>|t|)"] <= 0.05, ]
  
  # Si aucun coefficient significatif, afficher un message
  if (length(significant_coeffs) == 0) {
    cat("No significant coefficients for this topic.\n\n")
  } else {
    print(significant_coeffs)
  }
  
  cat("\n---\n")
}

```

