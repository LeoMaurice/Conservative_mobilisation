---
title: "analyse_lexico"
format: html
editor: source
---

```{r}
if (!require("pacman")) install.packages("pacman"); library(pacman)
pacman::p_load(tidyverse,
               lubridate,
               here,
               knitr,
               quanteda,
               quanteda.textstats,
               stm,
               topicmodels,
               LDAvis,
               ldatuning,
               readr,
               quanteda.textplots,
               ggprism,
               ggpmisc,
               RJSONIO,
               ggrepel,
               ggcorrplot,
               pdftools,
               #pdftk, pas dispo
               #tabulizer,
               tesseract,
               stringr,
               openxlsx,
               readr,
               readxl
            )

# Some little helps from the internet
source("../src/helpers/lda_reports.R")

seed = 24021956 # date de naissance de Butler
set.seed(seed)
```

```{r}
# modifié ID, mettre les ID mis en place par Fynch

femelliste_articles <- read_delim("../data/text/femelliste_articles.tsv", 
    delim = "\t", escape_double = FALSE, 
    trim_ws = TRUE)|>
  rename(Texte = "Texte Complet",
         URL = "URL Complète")|>
  mutate(Origine = "Femelliste",
         ID = row_number())


femelliste_articles$Texte = femelliste_articles$Texte|>
  str_replace_all("\\s+", " ") |>
  str_replace_all("[']", " ") |>
  str_replace_all("[’]", " ") |>
  str_replace_all("[`]", " ") |>
  str_replace_all("\r", " ") |>
  str_replace_all("\n", " ") |>
  str_replace_all("[,]",".")

cp <- corpus(femelliste_articles$Texte, 
             docvars = femelliste_articles |> select(ID,
                                    Origine, Date) |> as.data.frame(), 
             docnames = femelliste_articles$ID)
# tokenisation
tk_original <- tokens(cp, remove_punct = TRUE, remove_numbers = TRUE)
```


```{r}
toremove <- c(stopwords("fr"), stopwords("en"),
              "plus",
              "comme",
              "fait",
              "ça",
              "tout",
              "avoir",
              "très",
              "si",
              "alors",
              "entre",
              "donc",
              "ans",
              "peut",
              "deux",
              "elles",
              "travail",
              "parce",
              "autres",
              "faire",
              "quand",
              "car",
              "temps",
              "réalité",
              "dit",
              "dont",
              "après",
              "dire",
              "fois",
              "aussi",
              "pouvez",
              "ainsi",
              "où",
              "monde",
              "plusieurs",
              "article",
              "chez",
              "certaines",
              "cas",
              "vie",
              "tous",
              "souvent",
              "toutes",
              "bien",
              "quelques",
              "surpatreon",
              "terme",
              "pourquoi",
              "également",
              ">",
              "autre",
              "peuvent",
              "sens",
              "encore",
              "faits",
              "ensemble",
              "toujours",
              "question",
              "non",
              "afin",
              "chose",
              "depuis",
              "commencé",
              "co-fondatrice",
              "beaucoup",
              "parmarguerite",
              "années")

tk <- tokens_remove(tk_original,toremove)

# DFM format
dfm <- dfm(tk) |>
  dfm_remove(toremove) |>
  dfm_trim(min_termfreq = 5)
```

```{r}
print(textstat_frequency(dfm, n=100))|> select(feature) |> kable()
```

# Stat desc

```{r}
#| eval: true
#| output: true
#| fig-cap: Mots les plus fréquents dans le corpus. La taille est proportionnelle à la fréquence.
#| label: fig-wordcloud
palette <-ggprism::prism_color_pal("winter_bright")(9)
textplot_wordcloud(dfm, min_count = 20, random_order = FALSE, rotation = 0.25,
                   color = palette)
```
# Choix nombre de topics

```{r}
#| eval: true
recompute_choix_K = FALSE
tm_data <- quanteda::convert(dfm, to = "topicmodels")
stm_data <- convert(dfm, to = "stm")


if(recompute_choix_K){
  list_nb_topics_K <- seq(5, 20)

  tp_nb <- FindTopicsNumber(tm_data, topics = list_nb_topics_K, 
                          metrics = c("Griffiths2004", "CaoJuan2009", 
                                      "Arun2010", "Deveaud2014"),
                          method = "Gibbs")

  diag <- searchK(stm_data$documents, stm_data$vocab, 
                  list_nb_topics_K, 
                  verbose=FALSE)
  save(tp_nb,list_nb_topics_K,diag, file = "../data/results/LDA_report_20231205_metrics.RData")
} else{
  load("../data/results/LDA_report_20231205_metrics.RData")
}

```

## Métriques

```{r}
#| eval: true
#| output: true
#| fig-cap: Evolution des différentes métriques en fonction du nombre de topics
#| fig-subcap: Commenter métriques/résultats
#| label: fig-lda-metrics

FindTopicsNumber_plot(tp_nb)+
  theme_prism()+
  scale_colour_prism("winter_bright")
  
```
## Exclusivité et cohérence sémantique
```{r}
#| eval: true
#| output: true
#| fig-cap: Cohérence sémantique et excluvité en fonction du nombre de topics
#| label: fig-lda-exclu_cohsem
map(diag$results, unlist) |> 
  bind_cols() |> 
  ggplot(aes(exclus, semcoh, label = K)) +
    geom_point() +
    geom_label() +
  theme_prism()
```

# STM
```{r}
num_topic = 7
```

```{r}
#| eval: true

stm_data <- convert(dfm, to = "stm")

stm_lda <- stm(dfm,
               K=num_topic, 
               #prevalence =~ ,
               #content =~ ,
               init.type = "Spectral",
               seed = seed, 
               verbose = FALSE)
```

```{r}
toLDAvis(stm_lda, stm_data$documents,reorder.topics = FALSE)

```
```{r}
labelTopics(stm_lda)

```
```{r}
plot(stm_lda, type = "summary")

```
```{r}
plot(stm_lda, type = "summary", labeltype = "frex", frexw=0.85)
```


