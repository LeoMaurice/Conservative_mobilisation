---
title: "analyse_lexico"
format: html
editor: source
---

```{r}
if (!require("pacman")) install.packages("pacman"); library(pacman)
pacman::p_load(tidyverse,
               lubridate,
               here,
               knitr,
               
               quanteda,
               quanteda.textstats,
               quanteda.textplots,

               tidytext,
                              
               stm,
               topicmodels,
               LDAvis,
               ldatuning,
               lmtest,
               
               ggprism,
               ggpmisc,
               RJSONIO,
               ggrepel,
               ggcorrplot,
               ggpubr,
               
               stringr,
               readr,
               readxl
            )

couleurs_ideologie <- ggprism::prism_colour_pal("colorblind_safe")(6)[-1]

# Some little helps from the internet
source("../src/helpers/lda_reports.R")

save_figure = T

seed = 24021956 # date de naissance de Butler
set.seed(seed)
```
```{r}
ggprism::prism_colour_pal("winter_bright")(9)

```


# Ouverture données
```{r}
Base_de_données_anti_trans <- read_excel("../data/Base de données anti trans.xlsx", 
    col_types = c("text", "text", "skip", 
        "skip", "text","text", "skip", "skip", "skip"))|>
  rename(ID_ASSO = Id,
         Asso = nom,
         Idéologie = Idéologie_simplifiée,
         Droite = placement_droite)|>
  mutate(Idéologie = as.factor(Idéologie),
         Droite = Droite=="VRAI")
```

```{r}
base_articles <- read_delim("../data/intermediate/base_lemmatized.csv", 
    delim = ";", escape_double = FALSE, col_types = cols(`ID ASSO` = col_character(), 
        `ID ARTICLE` = col_character(), contenu = col_character(), 
        `Type de document` = col_character(), 
        Auteur = col_character(), Date = col_datetime(format = "%d/%m/%Y"), 
        Titre = col_character(), URL = col_character(), 
        lemmatized_contenu = col_character()), 
    trim_ws = TRUE)|>
  select(-URL)|>
  rename(ID = "ID ARTICLE",
         ID_ASSO = "ID ASSO",
         text = "lemmatized_contenu",
         Type_doc = "Type de document")|>
  left_join(Base_de_données_anti_trans, by = "ID_ASSO")

```


```{r}
base_articles$text = base_articles$text|>
  str_replace_all("\\s+", " ") |>
  str_replace_all("[']", " ") |>
  str_replace_all("[’]", " ") |>
  str_replace_all("[`]", " ") |>
  str_replace_all("\r", " ") |>
  str_replace_all("\n", " ") |>
  str_replace_all("[,]",".")
```

```{r}
base_articles$Asso|> table()
```


```{r}
base_articles|>
  group_by(ID_ASSO)|>
  slice_sample(n = 1)|>
  ungroup()

```

## tokenisation
```{r}
keywords <- c("trans", "transexuel", "transexuelle", "transexuels", "transexuelles",
              "transgenre", "transgenres", "transidentité", "transsexualité", "transsexualités",
              "transgender", "non-binaire", "non-binaires", "genderqueer", "intersexe", "intersexes",
              "queer", "affirmant le genre", "affirmant les genres", "transition de genre",
              "transitions de genre", "identité de genre", "identités de genre")


# Construction d'une expression régulière à partir des mots-clés pour la recherche
regex_keywords <- paste(keywords, collapse = "|")

# Filtrage des lignes du data.frame basé sur la présence d'au moins un mot-clé dans 'text'
# filtered_data <- base_articles |>
#   filter(grepl(regex_keywords, text, ignore.case = TRUE))

# Function to count occurrences of keywords in a text
count_keywords <- function(text, keywords) {
  counts <- sapply(keywords, function(keyword) {
    sum(grepl(keyword, text, ignore.case = TRUE))
  })
  total_count <- sum(counts)
  return(total_count)
}

# Filter based on the count of occurrences of keywords
filtered_data <- base_articles %>%
  filter(sapply(.$text, count_keywords, keywords) >= 1)

# one hot encoder
filtered_data <- filtered_data|>
  mutate(ED = Idéologie=="ED",
         FE = Idéologie=="FE",
         XD = Idéologie=="XD",
         CT = Idéologie=="CT")

cp <- corpus(filtered_data$text, 
             docvars = filtered_data |> select(Asso, Date, Type_doc,Idéologie, ED,FE,XD,CT,Droite) |> as.data.frame(), 
             docnames = filtered_data$ID)

# too frequent words
too_frequent_words <- c("femme","homme","personne","sexe","genre")
# words to remove
toremove <- readLines("../data/intermediate/words_to_filter.txt")
toremove <- c(stopwords("en"),stopwords("fr"),toremove,
              too_frequent_words)

# tokenisation
tk <- tokens(cp, remove_punct = TRUE, remove_numbers = TRUE)
tk <- tokens_remove(tk,toremove)
tk <- tokens_replace(tk, pattern = "tran", replacement = "trans", valuetype = "fixed") # effect of spacy tokenization

dfm <- dfm(tk) |>
  dfm_remove(toremove) |>
  dfm_trim(min_termfreq = 5)
```

```{r}
textstat_frequency(dfm, n=100)
```


# Stat desc 

## tableau d'occurences des associations et des idéologies

```{r}
# Calculer les nombres d'occurence
occurence_asso <- table(filtered_data$Asso) |> 
  as.data.frame() |>
  rename(Association = Var1, Occurences = Freq) |>
  arrange(desc(Occurences))

latex_table <- kable(occurence_asso, format = "latex", caption = "Occurences des Associations")

# Sauvegarder le tableau LaTeX dans un fichier
if(save_figure){
  writeLines(latex_table, "../output/descriptive/tableau_occurence_asso.tex")
}

kable(occurence_asso, format = "html", caption = "Occurences des Associations")
```

```{r}
# Calculer les nombres d'occurence
occurence_asso <- table(filtered_data$Idéologie) |> 
  as.data.frame() |>
  rename(Idéologie = Var1, Occurences = Freq) |>
  arrange(desc(Occurences))

latex_table <- kable(occurence_asso, format = "latex", caption = "Occurences des idéologies")

# Sauvegarder le tableau LaTeX dans un fichier
if(save_figure){
  writeLines(latex_table, "../output/descriptive/tableau_occurence_idéo.tex")
}

kable(occurence_asso, format = "html", caption = "Occurences des Associations")
```

```{r}
hist_occurences_doc <- ggplot(occurence_asso, aes(x = Idéologie, y = Occurences, fill = Idéologie)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "Nombre de documents par idéologie",
       x = "Idéologie", y = "Occurences") +
  theme_pubclean() +
  scale_fill_manual(values = couleurs_ideologie) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1))

if(save_figure){
  ggsave("../output/descriptive/hist_occurences_doc_ideologies.pdf", plot = hist_occurences_doc)
}

hist_occurences_doc

```
```{r}
occurence <- filtered_data|>
  group_by(Asso,Droite)|>
  summarise(Occurences = n())|>
  ungroup()|>
  arrange(-Occurences)
occurence$Asso <- reorder(occurence$Asso, occurence$Occurences)
hist_asso_droite <- ggplot(occurence, aes(x = Asso, y = Occurences, fill = Droite)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "Nombre de documents par Association",
       x = "Idéologie", y = "Occurences") +
  theme_pubclean() +
  coord_flip()+
  scale_fill_manual(values = c("#FF0066","#000000"))

if(save_figure){
  ggsave("../output/descriptive/hist_asso_droite.pdf", plot = hist_asso_droite)
}

hist_asso_droite
```

## Frequency

```{r}
most_frequent_words_plot <- dfm |> 
  textstat_frequency(n = 15) |> 
  ggplot(aes(x = reorder(feature, frequency), y = frequency)) +
  geom_point() +
  coord_flip() +
  labs(x = NULL, y = "Occurences") +
  theme_minimal()

if(save_figure){
  ggsave("../output/descriptive/most_frequent_words.pdf", plot = most_frequent_words_plot)
}
most_frequent_words_plot
```


```{r}
#| eval: true
#| output: true
#| fig-cap: Mots les plus fréquents dans le corpus. La taille est proportionnelle à la fréquence.
#| label: fig-wordcloud
palette <-ggprism::prism_color_pal("winter_bright")(9)
wordcloud_plot <- textplot_wordcloud(dfm, min_count = 500, random_order = FALSE, rotation = 0.25,
                   color = "#000080")
# if(save_figure){
#   ggsave("../output/descriptive/wordcloud_most_frequent_words.pdf", plot = wordcloud_plot)
# }

wordcloud_plot
```
## TF IDF

```{r}
tidy_dfm <- tidy(dfm)|>
  bind_tf_idf(term,document,count)|>
  rename(ID = document)|>
  left_join(dfm@docvars|>
             rename(ID = docid_)|>
             select(ID,Idéologie), by = "ID")
tidy_dfm|>
  group_by(Idéologie,term)|>
  summarise(tf_idf = mean(tf_idf))|>
  arrange(-tf_idf)|>
  group_by(Idéologie)|>
  top_n(10)|>
  ungroup()|>
  mutate(term = reorder_within(term, tf_idf, Idéologie)) |>
  ggplot(aes(term, tf_idf, fill = Idéologie)) +
  geom_col(alpha = 0.8, show.legend = FALSE) +
  facet_wrap(~ Idéologie, scales = "free", ncol = 2) +
  scale_x_reordered() +
  coord_flip() +
  theme(strip.text=element_text(size=11)) +
  labs(x = NULL, y = "tf-idf",
       title = "Top 10 mean TF-IDF par Idéologies")+
  theme_pubclean()+
  scale_fill_manual(values = couleurs_ideologie)
  
```
```{r}
tidy(dfm)|>
  rename(ID = document)|>
  left_join(dfm@docvars|>
             rename(ID = docid_)|>
             select(ID,Idéologie), by = "ID")|>
  group_by(term,Idéologie,count)|>
  summarise()|>
  bind_tf_idf(term,Idéologie,count)|>
  arrange(-tf_idf)|>
  group_by(Idéologie)|>
  top_n(10)|>
  ungroup()|>
  mutate(term = reorder_within(term, tf_idf, Idéologie)) |>
  ggplot(aes(term, tf_idf, fill = Idéologie)) +
  geom_col(alpha = 0.8, show.legend = FALSE) +
  facet_wrap(~ Idéologie, scales = "free", ncol = 2) +
  scale_x_reordered() +
  coord_flip() +
  theme(strip.text=element_text(size=11)) +
  labs(x = NULL, y = "tf-idf",
       title = "Top 10 TF-IDF par Idéologies")+
  theme_pubclean()+
  scale_fill_manual(values = couleurs_ideologie)
  
```
## Fréquence certains mots par idéologie
```{r}
# Liste des mots clés
keywords <- c("enfant", "corps", "religion", "lesbien","prostitution","transition","féministe")

# Filtrer le DFM pour ne garder que les mots clés
dfm_keywords <- dfm_select(dfm, pattern = keywords, selection = "keep")

# Convertir le DFM filtré en dataframe pour l'analyse
df_keywords <- convert(dfm_keywords, to = "data.frame")|>
  rename(ID = doc_id)|>
  left_join(filtered_data|>
              mutate(n_words=nchar(text))|>
              select(ID,n_words,Idéologie), by = "ID")

# # Ajouter la covariable 'type_doc' depuis votre dataframe 'data'
# df_keywords$Idéologie <- filtered_data$Idéologie

# Calculer la moyenne des fréquences pour chaque mot clé par Idéologie
df_freq <- df_keywords |> 
  mutate(across(keywords,~ 1000 * .x / n_words))|>
  group_by(Idéologie) |>
  summarise(across(keywords, mean, na.rm = TRUE)) |>
  pivot_longer(-Idéologie, names_to = "Mot", values_to = "Fréquences")

hist_freq_keywords <- ggplot(df_freq, aes(x = Mot, y = Fréquences, fill = Idéologie)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "Fréquence des mots clés par idéologie",
       x = "Mot clé", y = "Fréquence moyenne par document (‰)") +
  theme_pubclean() +
  scale_fill_manual(values = couleurs_ideologie) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1))
  

if(save_figure){
  ggsave("../output/descriptive/hist_freq_keywords_ideologies.pdf", plot = hist_freq_keywords)
}

hist_freq_keywords
```

```{r}
keywords <- c("trans","transgenre", "transidentité","transgender", 
              "queer")

# Filtrer le DFM pour ne garder que les mots clés
dfm_keywords <- dfm_select(dfm, pattern = keywords, selection = "keep")

# Convertir le DFM filtré en dataframe pour l'analyse
df_keywords <- convert(dfm_keywords, to = "data.frame")|>
  rename(ID = doc_id)|>
  left_join(filtered_data|>
              mutate(n_words=nchar(text))|>
              select(ID,n_words,Idéologie), by = "ID")

# Calculer la moyenne des fréquences pour chaque mot clé par Idéologie
df_freq <- df_keywords |> 
  mutate(across(keywords,~ .x))|>
  group_by(Idéologie) |>
  summarise(across(keywords, mean, na.rm = TRUE)) |>
  pivot_longer(-Idéologie, names_to = "Mot", values_to = "Fréquences")

hist_occurences_filtre <- ggplot(df_freq, aes(x = Mot, y = Fréquences, fill = Idéologie)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "Occurences des mots du filtre par idéologie",
       x = "Mot clé", y = "Occurences moyenne par document") +
  theme_pubclean() +
  scale_fill_manual(values = couleurs_ideologie) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1))+
  geom_hline(yintercept = 1, linetype = "dashed")+
  geom_hline(yintercept = 5, linetype = "dashed")+
  geom_hline(yintercept = 3, linetype = "dashed")

if(save_figure){
  ggsave("../output/descriptive/hist_occurences_filtre.pdf", plot = hist_occurences_filtre)
}
hist_occurences_filtre
```



```{r}
# Filtrer le DFM pour ne garder que les mots clés
dfm_keywords <- dfm_select(dfm, pattern = keywords, selection = "keep")

# Convertir le DFM filtré en dataframe pour l'analyse
df_keywords <- convert(dfm_keywords, to = "data.frame")|>
  rename(ID = doc_id)|>
  left_join(filtered_data|>
              mutate(n_words=nchar(text))|>
              select(ID,n_words,Idéologie), by = "ID")

# Calculer la moyenne des fréquences pour chaque mot clé par Idéologie
df_freq <- df_keywords |> 
  mutate(across(keywords,~ 1000 * .x / n_words))|>
  select(any_of(keywords),Idéologie)|>
  pivot_longer(-Idéologie, names_to = "Mot", values_to = "Fréquences")

boxplot_freq_keywords <- ggboxplot(data=df_freq, x = "Mot", y = "Fréquences", facet.by = "Idéologie", fill = "Idéologie") +
  labs(title = "Fréquence des mots clés par idéologie",
       x = "Mot clé", y = "Fréquence moyenne par document (‰)") +
  theme_pubclean() +
  scale_fill_manual(values = couleurs_ideologie) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1))
  

if(save_figure){
  ggsave("../output/descriptive/boxplot_freq_keywords.pdf", plot = boxplot_freq_keywords)
}

boxplot_freq_keywords
```


# Choix nombre de topics

```{r}
#| eval: true
recompute_choix_K = FALSE
tm_data <- quanteda::convert(dfm, to = "topicmodels")
stm_data <- convert(dfm, to = "stm")


if(recompute_choix_K){
  list_nb_topics_K <- seq(2, 40, 2)

  tp_nb <- FindTopicsNumber(tm_data, topics = list_nb_topics_K, 
                          metrics = c("Griffiths2004", "CaoJuan2009", 
                                      "Arun2010", "Deveaud2014"),
                          method = "Gibbs")

  diag <- searchK(stm_data$documents, stm_data$vocab, 
                  list_nb_topics_K, 
                  verbose=FALSE)
  save(tp_nb,list_nb_topics_K,diag, file = "../data/results/LDA_report_20240222_metrics_40.RData")
} else{
  load("../data/results/LDA_report_20240222_metrics.RData")
}

```

## Métriques

```{r}
#| eval: true
#| output: true
#| fig-cap: Evolution des différentes métriques en fonction du nombre de topics
#| fig-subcap: Commenter métriques/résultats
#| label: fig-lda-metrics

FindTopicsNumber_plot(tp_nb)

```

```{r}
tp_nb_custom <- tp_nb|>
  pivot_longer(cols=c("Griffiths2004","CaoJuan2009","Arun2010","Deveaud2014"), names_to = "metricsType",values_to = "metrics")|>
  mutate(category = ifelse(metricsType %in% c("Arun2010","CaoJuan2009"),"minimize","maximize"))|>
  group_by(metricsType)|>
  mutate(max = max(metrics),
         min = min(metrics))|>
  mutate(metrics = (metrics-min)/(max-min))|>
  ungroup()
  

custom_tp_nb_plot <- ggplot(tp_nb_custom, aes(x = topics, y = metrics, color = metricsType, group = metricsType, shape = metricsType)) +
  geom_line() +
  geom_point() +
  facet_wrap(~category, ncol = 1) + # Separate plots for Minimize vs Maximize
  scale_color_prism(palette = "winter_bright") +
  theme_pubclean() + # Use ggpubr theme
  labs(x = "number of topics", y = NULL, color = "metrics:",shape = "metrics:") +
  theme(legend.position = "right",
        legend.title = element_text(),
        panel.grid.major.x = element_line(linewidth = 0.2,color = "grey"),
        panel.grid.minor.x = element_line(linewidth = 0.1,color = "grey"))
if(save_figure){
  ggsave("../output/lda/find_topic_number_metrics.pdf",plot = custom_tp_nb_plot)
}
custom_tp_nb_plot
```

## Exclusivité et cohérence sémantique
```{r}
#| eval: true
#| output: true
#| fig-cap: Cohérence sémantique et excluvité en fonction du nombre de topics
#| label: fig-lda-exclu_cohsem
semcoh_exclu_plot<- map(diag$results, unlist) |> 
  bind_cols() |> 
  ggplot(aes(exclus, semcoh, label = K)) +
  labs(y = "cohérence sémantique",x = "exclusivité")+
    geom_point() +
    geom_label() +
  theme_pubclean()
if(save_figure){
  ggsave("../output/lda/semcoh_exlu.pdf",plot = semcoh_exclu_plot)
}
semcoh_exclu_plot
```

# STM
## Model
```{r}
num_topic = 6
#formula = as.formula("~ Droite + factor(Idéologie)") 
#formula = as.formula("~ Droite") 
#formula = as.formula("~ factor(Idéologie)") 
formula = as.formula("~ ED + XD + CT")

```

```{r}
#| eval: true

stm_data <- convert(dfm, to = "stm")

stm_lda <- stm(dfm,
               K=num_topic, 
               prevalence = formula, #+ factor(Asso),
               #content =~ Type_doc ,
               init.type = "Spectral",
               seed = seed, 
               verbose = FALSE)
```

## Exploration topics

```{r}
# toLDAvis(stm_lda, stm_data$documents,reorder.topics = FALSE)

```
```{r}
labelTopics(stm_lda)

```
```{r}
plot(stm_lda, type = "summary", labeltype = "prob")
# if(save_figure){
#   png("../output/lda/label_prob.png")
#   dev.off()
# }
```
```{r}
plot(stm_lda, type = "labels", labeltype = "prob", frexw=0.5)
# if(save_figure){
#   pdf("../output/lda/top20_prob.pdf")
#   dev.off()
# }

```

```{r}
plot(stm_lda, type = "summary", labeltype = "frex")
# if(save_figure){
#   pdf("../output/lda/label_frex.pdf")
#   dev.off()
# }
```
```{r}
plot(stm_lda, type = "labels", labeltype = "frex", frexw=0.5)
# if(save_figure){
#   pdf("../output/lda/top20_frex.pdf")
#   dev.off()
# }

```

```{r}
plot(stm_lda, type = "perspectives", topics = c(1,3))

```

```{r}
corrmat <- topicCorr(stm_lda)
plot(corrmat)
```
## Figures
```{r}
tidy_model <- tidy(stm_lda)

figure_top20_prob<- tidy_model |>
    group_by(topic) |>
    top_n(6, beta) |>
    ungroup() |>
    mutate(topic = paste0("Topic ", topic),
           term = reorder_within(term, beta, topic)) |>
    ggplot(aes(term, beta, fill = as.factor(topic))) +
    geom_col(alpha = 0.8, show.legend = FALSE) +
    facet_wrap(~ topic, scales = "free_y") +
    coord_flip() +
    scale_x_reordered() +
    labs(x = NULL, y = expression(beta),
         title = "Probabilités de mots les plus élevés pour chaque thème",
         subtitle = "Différents mots sont associés à différents sujets")+
  theme_pubclean()+
  theme(axis.text.y = element_text(size = 8))
if(save_figure){
  ggsave("../output/lda/figure_top20_words.pdf",plot =figure_top20_prob)
}
figure_top20_prob
```
```{r}
td_gamma <- tidy(stm_lda, matrix = "gamma")

prevalence_doc<-ggplot(td_gamma, aes(gamma, fill = as.factor(topic))) +
  geom_histogram(alpha = 0.8, show.legend = FALSE) +
  facet_wrap(~ topic, ncol = 3) +
  labs(title = "Distribution des probabilités des documents pour chaque thème",
       subtitle = "Chaque thème est associé à quelques documents",
       y = "Nombre de document", x = expression(gamma))+
  theme_pubclean()+
  theme(panel.grid.major.y = element_line(linewidth = 0.2,color = "grey"),
        panel.grid.minor.y = element_line(linewidth = 0.1,color = "grey"))
if(save_figure){
  ggsave("../output/lda/prevalence_doc.pdf",plot = prevalence_doc)
}
prevalence_doc
```

## Effet des métadonnées

```{r}
# Supposons que 'effets' est l'objet retourné par estimateEffect
stm_effets <- estimateEffect(formula = 1:num_topic ~ ED + XD + CT, 
                         stmobj = stm_lda, metadata = stm_data$meta, uncertainty = "Global")

# Extraction des résumés des effets pour chaque sujet
summaries <- summary(stm_effets, topics = 1:num_topic)

# Pour chaque sujet, filtrer et afficher uniquement les coefficients significatifs

cat("\n\n\n\n")
for (i in 1:num_topic) {
  cat("Topic", i, ":\n\n")
  
  # Extrait les coefficients et p-valeurs pour le sujet courant
  coeffs <- summaries$tables[[i]]
  
  # Filtrer pour garder uniquement les coefficients significatifs
  significant_coeffs <- coeffs[coeffs[, "Pr(>|t|)"] <= 0.1, ]
  
  # Si aucun coefficient significatif, afficher un message
  if (length(significant_coeffs) == 0) {
    cat("No significant coefficients for this topic.\n\n")
  } else {
    print(significant_coeffs)
  }
  
  cat("\n---\n")
}

```

```{r}
# Exécuter un test de permutation pour chaque sujet par rapport à la variable 'Idéologie'

results <- permutationTest(formula = 1:num_topic ~ ED + FE + XD,
                           stmobj = stm_lda,
                           data = dfm@docvars,
                           treatment = "ED"
                                  )  # Nombre de permutations, à ajuster selon les besoins

```

