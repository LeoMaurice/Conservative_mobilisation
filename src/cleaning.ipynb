{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "compute_freq = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce code réalise une lemmatization à partir d'un modèle de machine learning issu de spacy.\n",
    "installation du modèle : \n",
    "> python -m spacy download fr_core_news_sm\n",
    "\n",
    "Ensuite on supprime certains mots fréquents, remis en forme et trouvés grâce à :\n",
    "> from nltk.probability import FreqDist\n",
    "\n",
    "Les nombres sont aussi filtrés.\n",
    "\n",
    "J'ai réalisé cette approche car un  cleaning manuel était trop long (code en n² où n est le nombre de mots total du corpus). Le défaut de cette aproche est que potentiellement est elle trop bourrine et certains mots spécifiques qui nous intéressent ont disparu.\n",
    "\n",
    "Il faut valider par une approche lexico et LDA les résultats => cf le code analyse_lexico.qmd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Opening data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID ASSO</th>\n",
       "      <th>ID ARTICLE</th>\n",
       "      <th>contenu</th>\n",
       "      <th>Type de document</th>\n",
       "      <th>Auteur</th>\n",
       "      <th>Date</th>\n",
       "      <th>Titre</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>13</td>\n",
       "      <td>130028</td>\n",
       "      <td>Róisín Murphy et la fatalité d’une position ra...</td>\n",
       "      <td>blog</td>\n",
       "      <td>TRADFEM</td>\n",
       "      <td>09/09/2023</td>\n",
       "      <td>Róisín Murphy et la fatalité d’une position ra...</td>\n",
       "      <td>https://tradfem.wordpress.com/2023/09/09/roisi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>20</td>\n",
       "      <td>200064</td>\n",
       "      <td>Vues par notre sensibilité sociétale actuelle,...</td>\n",
       "      <td>Article d'opinion</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://cryforrecognition.be/fr/confusion-entr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467</th>\n",
       "      <td>13</td>\n",
       "      <td>130097</td>\n",
       "      <td>Déclaration de Mme Reem Alsalem, Rapporteuse s...</td>\n",
       "      <td>blog</td>\n",
       "      <td>TRADFEM</td>\n",
       "      <td>22/05/2023</td>\n",
       "      <td>Déclaration de Mme Reem Alsalem, Rapporteuse s...</td>\n",
       "      <td>https://tradfem.wordpress.com/2023/05/22/12813/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>13</td>\n",
       "      <td>130049</td>\n",
       "      <td>Trop de fierté annonce la chute : Pourquoi j’a...</td>\n",
       "      <td>blog</td>\n",
       "      <td>TRADFEM</td>\n",
       "      <td>20/07/2023</td>\n",
       "      <td>Trop de fierté annonce la chute : Pourquoi j’a...</td>\n",
       "      <td>https://tradfem.wordpress.com/2023/07/20/trop-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>6</td>\n",
       "      <td>60003</td>\n",
       "      <td>Depuis la découverte de “transgenres” dans plu...</td>\n",
       "      <td>Article d'opinion</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://laportelatine.org/formation/crise-egli...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ID ASSO  ID ARTICLE                                            contenu  \\\n",
       "398       13      130028  Róisín Murphy et la fatalité d’une position ra...   \n",
       "292       20      200064  Vues par notre sensibilité sociétale actuelle,...   \n",
       "467       13      130097  Déclaration de Mme Reem Alsalem, Rapporteuse s...   \n",
       "419       13      130049  Trop de fierté annonce la chute : Pourquoi j’a...   \n",
       "203        6       60003  Depuis la découverte de “transgenres” dans plu...   \n",
       "\n",
       "      Type de document   Auteur        Date  \\\n",
       "398               blog  TRADFEM  09/09/2023   \n",
       "292  Article d'opinion      NaN         NaN   \n",
       "467               blog  TRADFEM  22/05/2023   \n",
       "419               blog  TRADFEM  20/07/2023   \n",
       "203  Article d'opinion      NaN         NaN   \n",
       "\n",
       "                                                 Titre  \\\n",
       "398  Róisín Murphy et la fatalité d’une position ra...   \n",
       "292                                                NaN   \n",
       "467  Déclaration de Mme Reem Alsalem, Rapporteuse s...   \n",
       "419  Trop de fierté annonce la chute : Pourquoi j’a...   \n",
       "203                                                NaN   \n",
       "\n",
       "                                                   URL  \n",
       "398  https://tradfem.wordpress.com/2023/09/09/roisi...  \n",
       "292  https://cryforrecognition.be/fr/confusion-entr...  \n",
       "467    https://tradfem.wordpress.com/2023/05/22/12813/  \n",
       "419  https://tradfem.wordpress.com/2023/07/20/trop-...  \n",
       "203  https://laportelatine.org/formation/crise-egli...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fichier = \"../data/intermediate/base_merged.csv\"\n",
    "\n",
    "df = pd.read_csv(fichier, sep=';', quotechar='\"')\n",
    "\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning and tokenisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization with Spacy\n",
    "\n",
    "We also do some first filtering with Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da023b1ffcb24114aa7495e56da60cd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID ASSO</th>\n",
       "      <th>ID ARTICLE</th>\n",
       "      <th>contenu</th>\n",
       "      <th>Type de document</th>\n",
       "      <th>Auteur</th>\n",
       "      <th>Date</th>\n",
       "      <th>Titre</th>\n",
       "      <th>URL</th>\n",
       "      <th>lemmatized_contenu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>23</td>\n",
       "      <td>230008</td>\n",
       "      <td>Dans un rapport parlementaire, deux députés dé...</td>\n",
       "      <td>Article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.observatoirepetitesirene.org/ruben...</td>\n",
       "      <td>rapport parlementaire député dénoncer stéréoty...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>20</td>\n",
       "      <td>200048</td>\n",
       "      <td>Il croyait en l’auto-identification avant que ...</td>\n",
       "      <td>Article d'opinion</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://cryforrecognition.be/fr/a-quel-point-n...</td>\n",
       "      <td>croire l’ auto-identification mode \\r\\n n’ nie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>13</td>\n",
       "      <td>130041</td>\n",
       "      <td>Entrevue de Kajsa Ekis Ekman à propos de son n...</td>\n",
       "      <td>blog</td>\n",
       "      <td>TRADFEM</td>\n",
       "      <td>06/08/2023</td>\n",
       "      <td>Entrevue de Kajsa Ekis Ekman à propos de son n...</td>\n",
       "      <td>https://tradfem.wordpress.com/2023/08/06/entre...</td>\n",
       "      <td>entrevue Kajsa Ekis Ekman propos ouvrage l’ ex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>31</td>\n",
       "      <td>310004</td>\n",
       "      <td>Homme et femme Il les créa !\\r\\n\\r\\nLe monde m...</td>\n",
       "      <td>Article d'opinion</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-02-01 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.civitas-institut.com/2021/02/01/la...</td>\n",
       "      <td>homme femme créa \\r\\n\\r\\n monde moderne préten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>30</td>\n",
       "      <td>300011</td>\n",
       "      <td>\\r\\nLe 14 juillet dernier, à 20h22, Trystan, p...</td>\n",
       "      <td>Article d'opinion</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-07-17 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.parti-de-la-france.fr/articles/On-...</td>\n",
       "      <td>\\r\\n juillet dernier 20h22 trystan père transg...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ID ASSO  ID ARTICLE                                            contenu  \\\n",
       "233       23      230008  Dans un rapport parlementaire, deux députés dé...   \n",
       "276       20      200048  Il croyait en l’auto-identification avant que ...   \n",
       "411       13      130041  Entrevue de Kajsa Ekis Ekman à propos de son n...   \n",
       "345       31      310004  Homme et femme Il les créa !\\r\\n\\r\\nLe monde m...   \n",
       "330       30      300011  \\r\\nLe 14 juillet dernier, à 20h22, Trystan, p...   \n",
       "\n",
       "      Type de document   Auteur                 Date  \\\n",
       "233            Article      NaN                  NaN   \n",
       "276  Article d'opinion      NaN                  NaN   \n",
       "411               blog  TRADFEM           06/08/2023   \n",
       "345  Article d'opinion      NaN  2021-02-01 00:00:00   \n",
       "330  Article d'opinion      NaN  2018-07-17 00:00:00   \n",
       "\n",
       "                                                 Titre  \\\n",
       "233                                                NaN   \n",
       "276                                                NaN   \n",
       "411  Entrevue de Kajsa Ekis Ekman à propos de son n...   \n",
       "345                                                NaN   \n",
       "330                                                NaN   \n",
       "\n",
       "                                                   URL  \\\n",
       "233  https://www.observatoirepetitesirene.org/ruben...   \n",
       "276  https://cryforrecognition.be/fr/a-quel-point-n...   \n",
       "411  https://tradfem.wordpress.com/2023/08/06/entre...   \n",
       "345  https://www.civitas-institut.com/2021/02/01/la...   \n",
       "330  https://www.parti-de-la-france.fr/articles/On-...   \n",
       "\n",
       "                                    lemmatized_contenu  \n",
       "233  rapport parlementaire député dénoncer stéréoty...  \n",
       "276  croire l’ auto-identification mode \\r\\n n’ nie...  \n",
       "411  entrevue Kajsa Ekis Ekman propos ouvrage l’ ex...  \n",
       "345  homme femme créa \\r\\n\\r\\n monde moderne préten...  \n",
       "330  \\r\\n juillet dernier 20h22 trystan père transg...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the French language model\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "# Define your list of words to exclude\n",
    "words_to_filter = [\n",
    "    'le', 'la', 'les', 'de', 'un', 'à', 'être', 'suis', 'es', 'est', 'sont', 'été', 'étais', 'était', 'étaient', 'serai', 'seras', 'sera', 'seront',\n",
    "    'et', 'en', 'avoir', 'ai', 'as', 'a', 'ont', 'eu', 'aurai', 'auras', 'aura', 'auront', 'avais', 'avait', 'avaient', 'que', 'pour',\n",
    "    'dans', 'ce', 'il', 'qui', 'ne', 'en', 'sur', 'se', 'pas', 'plus',\n",
    "    'par', 'je', 'avec', 'tout', 'faire', 'fais', 'fait', 'font', 'son', 'mettre', 'autre', 'on', 'mais',\n",
    "    'nous', 'comme', 'ou', 'si', 'leur', 'y', 'dire', 'dis', 'dit', 'disent', 'elle', 'voir', 'devoir',\n",
    "    'deux', 'même', 'pendant', 'aussi', 'vouloir', 'grand', 'mon', 'nouveau', 'aller',\n",
    "    'venir', 'ceux', 'faire', 'quelque', 'trouver', 'donner', 'donne', 'donnes', 'donnent', 'aussi', 'autre',\n",
    "    'tous', 'vieux', 'bon', 'voir', 'moins', 'trois', 'avant', 'sa', 'faire', 'contre',\n",
    "    'abord', 'sous', 'ou', 'apporter', 'grand', 'ainsi', 'long', 'très', 'tout', 'avoir',\n",
    "    'beau', 'chaque', 'peu', 'quoi', 'encore', 'aller', 'montrer', 'semaine', 'ainsi',\n",
    "    'nuit', 'aussi', 'bien', 'deuxième', 'moins', 'tout', 'avoir', 'peu', 'nuit', 'ça',\n",
    "    'mon', 'ma', 'mes', 'ton', 'ta', 'tes', 'son', 'sa', 'ses', 'notre', 'notre', 'nos',\n",
    "      'votre', 'votre', 'vos', 'leur', 'leur', 'leurs',\n",
    "    'non', '’', '.', ',', ':', 'est', 'des', \"l'\", \"d'\", \"une\", \"d\",\n",
    "      \"»\", \"«\", \"du\", \"qu\", \"au\", \"n\", \"s\", \"elle\", \")\", \"(\", \"aux\", 'c', 'cette',\n",
    "        'un', 'ces','?', 'il', 'ils', 'j', \"l\", \"-\", \"vous\", \"entre\",\n",
    "    \"[\", \"]\", \"elles\", '”', '“', \"dont\", \"cela\", \";\", \"m\", \"selon\", \"!\", '–',\n",
    "      \"en\", \"autre\", \"ici\", \"ce\", \"ça\", \"cela\", \"ceci\", \"car\", \"ou\", \"où\",\n",
    "        \"ni\", \"mais\", \"et\", \"donc\", \"parce\", \"que\", \"quand\", \"comment\",\n",
    "          \"qui\", \"quoi\",\n",
    "    \"à\", \"de\", \"pour\", \"sans\", \"sur\",\"l\",\"d\",\"que\",\"lui\",\"n\",\"s\",\"c\",\"j\",\"me\",\"alors\",\"certain\",\"chez\",\"celer\"\n",
    "    \"lequel\",\"m\",\"cas\",\"tel\",\"luire\",\"après\",\"souvent\",\"aucun\",\"toujours\",\"également\",\"jamais\",\"hui\",\"beaucoup\",\"rien\"\n",
    "]\n",
    "\n",
    "# Function to tokenize, filter, and lemmatize text, excluding numbers\n",
    "def tokenize_filter_and_lemmatize(text):\n",
    "    # Process the text using spaCy\n",
    "    doc = nlp(text)\n",
    "    # Extract lemmatized forms of the words, excluding specific tokens and numbers\n",
    "    lemmatized = \" \".join([token.lemma_ for token in doc if token.lemma_ not in words_to_filter and not token.is_digit])\n",
    "    return lemmatized\n",
    "\n",
    "# Apply the function to the 'contenu' column\n",
    "df['lemmatized_contenu'] = df['contenu'].progress_apply(tokenize_filter_and_lemmatize)\n",
    "\n",
    "# Print the dataframe to see the result\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking Words frequency according to Spacy tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dbc4af77c5647faac266978feca9b99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 most frequent tokens:\n",
      "  : 7082\n",
      "femme: 5329\n",
      "pouvoir: 4187\n",
      "genre: 3024\n",
      "\n",
      " : 2723\n",
      "enfant: 2526\n",
      "homme: 2447\n",
      "personne: 2371\n",
      "sexe: 2016\n",
      "\n",
      " : 2011\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import spacy\n",
    "\n",
    "if compute_freq:\n",
    "    # Load the French language model\n",
    "    nlp = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "    # Your list of words to filter and the tokenize_filter_and_lemmatize function should be defined here\n",
    "\n",
    "    # Initialize a Counter object to hold the frequency of each token\n",
    "    token_freq = Counter()\n",
    "\n",
    "    # Function to update token frequency for a single document\n",
    "    def update_token_frequency(text):\n",
    "        global token_freq  # Reference the global Counter object\n",
    "        # Process the text with spaCy\n",
    "        doc = nlp(text)\n",
    "        # Update the Counter with tokens from this document, excluding filtered words and numbers\n",
    "        token_freq.update([token.text for token in doc if token.lemma_ not in words_to_filter and not token.is_digit])\n",
    "\n",
    "    # Apply the function to each row in the 'lemmatized_contenu' column to update the global token frequency\n",
    "    df['lemmatized_contenu'].progress_apply(update_token_frequency)\n",
    "\n",
    "    # Find the top 10 most frequent tokens\n",
    "    top_10_tokens = token_freq.most_common(10)\n",
    "\n",
    "    # Print the top 10 most frequent tokens\n",
    "    print(\"Top 10 most frequent tokens:\")\n",
    "    for token, freq in top_10_tokens:\n",
    "        print(f\"{token}: {freq}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second filtering with nltk\n",
    "\n",
    "the tokenization of spacy is better than the one I used in R (we do the LDA in R). The one in R is equivalent to the one of nltk. And for instance aujourd'hui appears as aujourd + ' + hui. So i need to run a second filter based on nltk tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\leopo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b9701f7f1bc48d782ecf3b6211e9c00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure NLTK resources are downloaded (needed for tokenization)\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Function to tokenize with NLTK and filter based on your criteria\n",
    "def nltk_tokenize_and_filter(text):\n",
    "    # Tokenize the text with NLTK, ensuring the text is treated as French\n",
    "    tokens = word_tokenize(text, language='french')\n",
    "    # Filter tokens: convert to lowercase, exclude if in words_to_filter or is a digit\n",
    "    filtered_tokens = [token.lower() for token in tokens if token.lower() not in words_to_filter and not token.isdigit()]\n",
    "    # Join the filtered tokens back into a string\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    return filtered_text\n",
    "\n",
    "# Assuming 'df' is your DataFrame and 'lemmatized_contenu' contains the text to process\n",
    "# Apply the function to each row in the 'lemmatized_contenu' column\n",
    "df['lemmatized_contenu'] = df['lemmatized_contenu'].progress_apply(nltk_tokenize_and_filter)\n",
    "\n",
    "# This results in a new column 'filtered_contenu' in your dataframe 'df' with the text processed as per your requirements\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking words frequency based on nltk tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'compute_freq' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Counter\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mcompute_freq\u001b[49m:\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# Ensure you've downloaded the NLTK tokenizer models\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     nltk\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpunkt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# Your DataFrame 'df' should already be loaded with the 'lemmatized_contenu' column ready\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \n\u001b[0;32m     12\u001b[0m     \u001b[38;5;66;03m# Initialize a Counter object for token frequencies\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'compute_freq' is not defined"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "if compute_freq:\n",
    "    # Ensure you've downloaded the NLTK tokenizer models\n",
    "    nltk.download('punkt')\n",
    "\n",
    "    # Your DataFrame 'df' should already be loaded with the 'lemmatized_contenu' column ready\n",
    "\n",
    "    # Initialize a Counter object for token frequencies\n",
    "    token_freq = Counter()\n",
    "\n",
    "    # Define your list of words to exclude, adjusted for your context\n",
    "    words_to_filter = set([\n",
    "        # Your list of words to filter\n",
    "    ])\n",
    "\n",
    "    # Function to tokenize and update token frequency for a single document using NLTK\n",
    "    def update_token_frequency_nltk(text):\n",
    "        global token_freq  # Reference the global Counter object\n",
    "        # Tokenize the text using NLTK\n",
    "        tokens = word_tokenize(text, language='french')\n",
    "        # Update the Counter with tokens from this document, excluding filtered words and numbers\n",
    "        token_freq.update([token.lower() for token in tokens if token.lower() not in words_to_filter and not token.isdigit()])\n",
    "\n",
    "    # Apply the function to each row in the 'lemmatized_contenu' column to update the global token frequency\n",
    "    df['lemmatized_contenu'].apply(update_token_frequency_nltk)\n",
    "\n",
    "    # Find the top 10 most frequent tokens\n",
    "    top_10_tokens = token_freq.most_common(10)\n",
    "\n",
    "    # Print the top 10 most frequent tokens\n",
    "    print(\"Top 10 most frequent tokens using NLTK tokenization:\")\n",
    "    for token, freq in top_10_tokens:\n",
    "        print(f\"{token}: {freq}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fichier = \"../data/intermediate/base_lemmatized.csv\"\n",
    "df.to_csv(fichier, sep=';', index=False, quotechar='\"')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ccne",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
