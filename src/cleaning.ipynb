{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "compute_freq = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce code réalise une lemmatization à partir d'un modèle de machine learning issu de spacy.\n",
    "installation du modèle : \n",
    "> python -m spacy download fr_core_news_sm\n",
    "\n",
    "Ensuite on supprime certains mots fréquents, remis en forme et trouvés grâce à :\n",
    "> from nltk.probability import FreqDist\n",
    "\n",
    "Les nombres sont aussi filtrés.\n",
    "\n",
    "J'ai réalisé cette approche car un  cleaning manuel était trop long (code en n² où n est le nombre de mots total du corpus). Le défaut de cette aproche est que potentiellement est elle trop bourrine et certains mots spécifiques qui nous intéressent ont disparu.\n",
    "\n",
    "Il faut valider par une approche lexico et LDA les résultats => cf le code analyse_lexico.qmd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Opening data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID ASSO</th>\n",
       "      <th>ID ARTICLE</th>\n",
       "      <th>contenu</th>\n",
       "      <th>Type de document</th>\n",
       "      <th>Auteur</th>\n",
       "      <th>Date</th>\n",
       "      <th>Titre</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>456</th>\n",
       "      <td>13</td>\n",
       "      <td>130086</td>\n",
       "      <td>Kathleen Stock se dit « modérée » alors que de...</td>\n",
       "      <td>blog</td>\n",
       "      <td>TRADFEM</td>\n",
       "      <td>31/05/2023</td>\n",
       "      <td>Kathleen Stock se dit « modérée » alors que de...</td>\n",
       "      <td>https://tradfem.wordpress.com/2023/05/31/kathl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>17</td>\n",
       "      <td>170038</td>\n",
       "      <td>Qu’il semble loin le jour où les femmes n’auro...</td>\n",
       "      <td>analyse</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>21</td>\n",
       "      <td>210027</td>\n",
       "      <td>Traduction de cet article écrit par Feminism I...</td>\n",
       "      <td>analyse</td>\n",
       "      <td>résistance lesbienne</td>\n",
       "      <td>2022-08-14</td>\n",
       "      <td>Quels choix personnels peuvent être critiqués?</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>12</td>\n",
       "      <td>120021</td>\n",
       "      <td>Le féminisme critique du genre, c’est quoi ?Le...</td>\n",
       "      <td>opinion</td>\n",
       "      <td>Dora Moutot &amp; Marguerite Stern</td>\n",
       "      <td>18/10/2022</td>\n",
       "      <td>Le féminisme critique du genre, c’est quoi ?</td>\n",
       "      <td>https://www.femelliste.com/articles-femellisme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>20</td>\n",
       "      <td>200078</td>\n",
       "      <td>Effets secondaires de la testostérone chez les...</td>\n",
       "      <td>Article d'opinion</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://cryforrecognition.be/fr/effets-seconda...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ID ASSO  ID ARTICLE                                            contenu  \\\n",
       "456       13      130086  Kathleen Stock se dit « modérée » alors que de...   \n",
       "66        17      170038  Qu’il semble loin le jour où les femmes n’auro...   \n",
       "157       21      210027  Traduction de cet article écrit par Feminism I...   \n",
       "20        12      120021  Le féminisme critique du genre, c’est quoi ?Le...   \n",
       "306       20      200078  Effets secondaires de la testostérone chez les...   \n",
       "\n",
       "      Type de document                          Auteur        Date  \\\n",
       "456               blog                         TRADFEM  31/05/2023   \n",
       "66             analyse                             NaN         NaN   \n",
       "157            analyse            résistance lesbienne  2022-08-14   \n",
       "20             opinion  Dora Moutot & Marguerite Stern  18/10/2022   \n",
       "306  Article d'opinion                             NaN         NaN   \n",
       "\n",
       "                                                 Titre  \\\n",
       "456  Kathleen Stock se dit « modérée » alors que de...   \n",
       "66                                                 NaN   \n",
       "157     Quels choix personnels peuvent être critiqués?   \n",
       "20        Le féminisme critique du genre, c’est quoi ?   \n",
       "306                                                NaN   \n",
       "\n",
       "                                                   URL  \n",
       "456  https://tradfem.wordpress.com/2023/05/31/kathl...  \n",
       "66                                                 NaN  \n",
       "157                                                NaN  \n",
       "20   https://www.femelliste.com/articles-femellisme...  \n",
       "306  https://cryforrecognition.be/fr/effets-seconda...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fichier = \"../data/intermediate/base_merged.csv\"\n",
    "\n",
    "df = pd.read_csv(fichier, sep=';', quotechar='\"')\n",
    "\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning and tokenisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization with Spacy\n",
    "\n",
    "We also do some first filtering with Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c99c91025e14c059376f4f7acbb12ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID ASSO</th>\n",
       "      <th>ID ARTICLE</th>\n",
       "      <th>contenu</th>\n",
       "      <th>Type de document</th>\n",
       "      <th>Auteur</th>\n",
       "      <th>Date</th>\n",
       "      <th>Titre</th>\n",
       "      <th>URL</th>\n",
       "      <th>lemmatized_contenu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>458</th>\n",
       "      <td>13</td>\n",
       "      <td>130088</td>\n",
       "      <td>La gauche a trahi les femmes en Espagne – elle...</td>\n",
       "      <td>blog</td>\n",
       "      <td>TRADFEM</td>\n",
       "      <td>31/05/2023</td>\n",
       "      <td>La gauche a trahi les femmes en Espagne – elle...</td>\n",
       "      <td>https://tradfem.wordpress.com/2023/05/31/la-ga...</td>\n",
       "      <td>gauche trahir femme espagne maintenant face co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>17</td>\n",
       "      <td>170006</td>\n",
       "      <td>Le 6 avril 2016 est un jour de victoire pour ...</td>\n",
       "      <td>analyse</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>avril jour victoire féministe allié progress...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>17</td>\n",
       "      <td>170031</td>\n",
       "      <td>La mobilisation internationale pour retrouver ...</td>\n",
       "      <td>analyse</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mobilisation international retrouver lycéen ni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>17</td>\n",
       "      <td>170028</td>\n",
       "      <td>Alors bien sûr il y a eu l’avis de la Commissi...</td>\n",
       "      <td>analyse</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sûr l’ avis commission droit l’ homme avis n’ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>17</td>\n",
       "      <td>170025</td>\n",
       "      <td>11 août 2014 actualisation\\n Dans ce texte, j...</td>\n",
       "      <td>analyse</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>août actualisation \\n  texte n’ prétention d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ID ASSO  ID ARTICLE                                            contenu  \\\n",
       "458       13      130088  La gauche a trahi les femmes en Espagne – elle...   \n",
       "34        17      170006   Le 6 avril 2016 est un jour de victoire pour ...   \n",
       "59        17      170031  La mobilisation internationale pour retrouver ...   \n",
       "56        17      170028  Alors bien sûr il y a eu l’avis de la Commissi...   \n",
       "53        17      170025   11 août 2014 actualisation\\n Dans ce texte, j...   \n",
       "\n",
       "    Type de document   Auteur        Date  \\\n",
       "458             blog  TRADFEM  31/05/2023   \n",
       "34           analyse      NaN         NaN   \n",
       "59           analyse      NaN         NaN   \n",
       "56           analyse      NaN         NaN   \n",
       "53           analyse      NaN         NaN   \n",
       "\n",
       "                                                 Titre  \\\n",
       "458  La gauche a trahi les femmes en Espagne – elle...   \n",
       "34                                                 NaN   \n",
       "59                                                 NaN   \n",
       "56                                                 NaN   \n",
       "53                                                 NaN   \n",
       "\n",
       "                                                   URL  \\\n",
       "458  https://tradfem.wordpress.com/2023/05/31/la-ga...   \n",
       "34                                                 NaN   \n",
       "59                                                 NaN   \n",
       "56                                                 NaN   \n",
       "53                                                 NaN   \n",
       "\n",
       "                                    lemmatized_contenu  \n",
       "458  gauche trahir femme espagne maintenant face co...  \n",
       "34     avril jour victoire féministe allié progress...  \n",
       "59   mobilisation international retrouver lycéen ni...  \n",
       "56   sûr l’ avis commission droit l’ homme avis n’ ...  \n",
       "53     août actualisation \\n  texte n’ prétention d...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the French language model\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "# Define your list of words to exclude\n",
    "with open(\"../data/intermediate/words_to_filter.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    words_to_filter = file.read().splitlines()\n",
    "\n",
    "tran_meaning = []\n",
    "\n",
    "# Function to tokenize, filter, and lemmatize text, excluding numbers\n",
    "def tokenize_filter_and_lemmatize(text):\n",
    "    # Process the text using spaCy\n",
    "    doc = nlp(text)\n",
    "    # Extract lemmatized forms of the words, excluding specific tokens and numbers\n",
    "    for token in doc:\n",
    "        if token.lemma_ == \"tran\":\n",
    "            tran_meaning.append(token.text)\n",
    "    lemmatized = \" \".join([token.lemma_ for token in doc if token.lemma_ not in words_to_filter and not token.is_digit])\n",
    "    return lemmatized\n",
    "\n",
    "# Apply the function to the 'contenu' column\n",
    "df['lemmatized_contenu'] = df['contenu'].progress_apply(tokenize_filter_and_lemmatize)\n",
    "\n",
    "# Print the dataframe to see the result\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'Trans',\n",
       " 'Trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'Trans',\n",
       " 'trans',\n",
       " 'Trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'Trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'Trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'Trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'Trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'TRANS',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'Trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'tran',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'tran',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'tran',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'TRANS',\n",
       " 'trans',\n",
       " 'TRANS',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'TRANS',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'TRANS',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'Trans',\n",
       " 'Trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans',\n",
       " 'trans']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tran_meaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking Words frequency according to Spacy tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import spacy\n",
    "\n",
    "if compute_freq:\n",
    "    # Load the French language model\n",
    "    nlp = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "    # Your list of words to filter and the tokenize_filter_and_lemmatize function should be defined here\n",
    "\n",
    "    # Initialize a Counter object to hold the frequency of each token\n",
    "    token_freq = Counter()\n",
    "\n",
    "    # Function to update token frequency for a single document\n",
    "    def update_token_frequency(text):\n",
    "        global token_freq  # Reference the global Counter object\n",
    "        # Process the text with spaCy\n",
    "        doc = nlp(text)\n",
    "        # Update the Counter with tokens from this document, excluding filtered words and numbers\n",
    "        token_freq.update([token.text for token in doc if token.lemma_ not in words_to_filter and not token.is_digit])\n",
    "\n",
    "    # Apply the function to each row in the 'lemmatized_contenu' column to update the global token frequency\n",
    "    df['lemmatized_contenu'].progress_apply(update_token_frequency)\n",
    "\n",
    "    # Find the top 10 most frequent tokens\n",
    "    top_10_tokens = token_freq.most_common(10)\n",
    "\n",
    "    # Print the top 10 most frequent tokens\n",
    "    print(\"Top 10 most frequent tokens:\")\n",
    "    for token, freq in top_10_tokens:\n",
    "        print(f\"{token}: {freq}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second filtering with nltk\n",
    "\n",
    "the tokenization of spacy is better than the one I used in R (we do the LDA in R). The one in R is equivalent to the one of nltk. And for instance aujourd'hui appears as aujourd + ' + hui. So i need to run a second filter based on nltk tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\leopo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54ee80db2e7c49779ea1a6860b9b634d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure NLTK resources are downloaded (needed for tokenization)\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Function to tokenize with NLTK and filter based on your criteria\n",
    "def nltk_tokenize_and_filter(text):\n",
    "    # Tokenize the text with NLTK, ensuring the text is treated as French\n",
    "    tokens = word_tokenize(text, language='french')\n",
    "    # Filter tokens: convert to lowercase, exclude if in words_to_filter or is a digit\n",
    "    filtered_tokens = [token.lower() for token in tokens if token.lower() not in words_to_filter and not token.isdigit()]\n",
    "    # Join the filtered tokens back into a string\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    return filtered_text\n",
    "\n",
    "# Assuming 'df' is your DataFrame and 'lemmatized_contenu' contains the text to process\n",
    "# Apply the function to each row in the 'lemmatized_contenu' column\n",
    "df['lemmatized_contenu'] = df['lemmatized_contenu'].progress_apply(nltk_tokenize_and_filter)\n",
    "\n",
    "# This results in a new column 'filtered_contenu' in your dataframe 'df' with the text processed as per your requirements\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking words frequency based on nltk tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "if compute_freq:\n",
    "    # Ensure you've downloaded the NLTK tokenizer models\n",
    "    nltk.download('punkt')\n",
    "\n",
    "    # Your DataFrame 'df' should already be loaded with the 'lemmatized_contenu' column ready\n",
    "\n",
    "    # Initialize a Counter object for token frequencies\n",
    "    token_freq = Counter()\n",
    "\n",
    "    # Define your list of words to exclude, adjusted for your context\n",
    "    words_to_filter = set([\n",
    "        # Your list of words to filter\n",
    "    ])\n",
    "\n",
    "    # Function to tokenize and update token frequency for a single document using NLTK\n",
    "    def update_token_frequency_nltk(text):\n",
    "        global token_freq  # Reference the global Counter object\n",
    "        # Tokenize the text using NLTK\n",
    "        tokens = word_tokenize(text, language='french')\n",
    "        # Update the Counter with tokens from this document, excluding filtered words and numbers\n",
    "        token_freq.update([token.lower() for token in tokens if token.lower() not in words_to_filter and not token.isdigit()])\n",
    "\n",
    "    # Apply the function to each row in the 'lemmatized_contenu' column to update the global token frequency\n",
    "    df['lemmatized_contenu'].apply(update_token_frequency_nltk)\n",
    "\n",
    "    # Find the top 10 most frequent tokens\n",
    "    top_10_tokens = token_freq.most_common(10)\n",
    "\n",
    "    # Print the top 10 most frequent tokens\n",
    "    print(\"Top 10 most frequent tokens using NLTK tokenization:\")\n",
    "    for token, freq in top_10_tokens:\n",
    "        print(f\"{token}: {freq}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fichier = \"../data/intermediate/base_lemmatized.csv\"\n",
    "df.to_csv(fichier, sep=';', index=False, quotechar='\"')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ccne",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
