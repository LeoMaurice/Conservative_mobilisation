{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "compute_freq = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce code réalise une lemmatization à partir d'un modèle de machine learning issu de spacy.\n",
    "installation du modèle : \n",
    "> python -m spacy download fr_core_news_sm\n",
    "\n",
    "Ensuite on supprime certains mots fréquents, remis en forme et trouvés grâce à :\n",
    "> from nltk.probability import FreqDist\n",
    "\n",
    "Les nombres sont aussi filtrés.\n",
    "\n",
    "J'ai réalisé cette approche car un  cleaning manuel était trop long (code en n² où n est le nombre de mots total du corpus). Le défaut de cette aproche est que potentiellement est elle trop bourrine et certains mots spécifiques qui nous intéressent ont disparu.\n",
    "\n",
    "Il faut valider par une approche lexico et LDA les résultats => cf le code analyse_lexico.qmd\n",
    "\n",
    "Parmi les effets négatifs : spacy lemmatize trans en tran => meh, on a donc dans le code R fusionné les token 'tran' et 'trans' mais on a aussi fixé ce problème en créant une expection dans la lemmatization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Opening data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID ASSO</th>\n",
       "      <th>ID ARTICLE</th>\n",
       "      <th>contenu</th>\n",
       "      <th>Type de document</th>\n",
       "      <th>Auteur</th>\n",
       "      <th>Date</th>\n",
       "      <th>Titre</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>21</td>\n",
       "      <td>210008</td>\n",
       "      <td>Traduction principalement DeepL d’un article p...</td>\n",
       "      <td>analyse</td>\n",
       "      <td>résistance lesbienne</td>\n",
       "      <td>2021-10-26</td>\n",
       "      <td>« Certaines femmes transgenres nous poussent à...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>17</td>\n",
       "      <td>170019</td>\n",
       "      <td>7 janvier 2015, 11h30, la rédaction de Charli...</td>\n",
       "      <td>analyse</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>12</td>\n",
       "      <td>120027</td>\n",
       "      <td>Quelle est la différence entre homosexualité e...</td>\n",
       "      <td>opinion</td>\n",
       "      <td>Marguerite Stern</td>\n",
       "      <td>09/10/2022</td>\n",
       "      <td>Quelle est la différence entre homosexualité e...</td>\n",
       "      <td>https://www.femelliste.com/articles-femellisme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>3</td>\n",
       "      <td>30004</td>\n",
       "      <td>Le pronom « iel » ou le sexe des anges\\nLa thé...</td>\n",
       "      <td>Article d'opinion</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.generation-zemmour.fr/le-pronom-ie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>17</td>\n",
       "      <td>170032</td>\n",
       "      <td>La sociologie étudie et explique la dimension...</td>\n",
       "      <td>analyse</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ID ASSO  ID ARTICLE                                            contenu  \\\n",
       "138       21      210008  Traduction principalement DeepL d’un article p...   \n",
       "47        17      170019   7 janvier 2015, 11h30, la rédaction de Charli...   \n",
       "26        12      120027  Quelle est la différence entre homosexualité e...   \n",
       "184        3       30004  Le pronom « iel » ou le sexe des anges\\nLa thé...   \n",
       "60        17      170032   La sociologie étudie et explique la dimension...   \n",
       "\n",
       "      Type de document                Auteur        Date  \\\n",
       "138            analyse  résistance lesbienne  2021-10-26   \n",
       "47             analyse                   NaN         NaN   \n",
       "26             opinion      Marguerite Stern  09/10/2022   \n",
       "184  Article d'opinion                   NaN         NaN   \n",
       "60             analyse                   NaN         NaN   \n",
       "\n",
       "                                                 Titre  \\\n",
       "138  « Certaines femmes transgenres nous poussent à...   \n",
       "47                                                 NaN   \n",
       "26   Quelle est la différence entre homosexualité e...   \n",
       "184                                                NaN   \n",
       "60                                                 NaN   \n",
       "\n",
       "                                                   URL  \n",
       "138                                                NaN  \n",
       "47                                                 NaN  \n",
       "26   https://www.femelliste.com/articles-femellisme...  \n",
       "184  https://www.generation-zemmour.fr/le-pronom-ie...  \n",
       "60                                                 NaN  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fichier = \"../data/intermediate/base_merged.csv\"\n",
    "\n",
    "df = pd.read_csv(fichier, sep=';', quotechar='\"')\n",
    "\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning and tokenisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization with Spacy\n",
    "\n",
    "We also do some first filtering with Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a5cabec88bf47648fa4bc44561d631a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID ASSO</th>\n",
       "      <th>ID ARTICLE</th>\n",
       "      <th>contenu</th>\n",
       "      <th>Type de document</th>\n",
       "      <th>Auteur</th>\n",
       "      <th>Date</th>\n",
       "      <th>Titre</th>\n",
       "      <th>URL</th>\n",
       "      <th>lemmatized_contenu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>13</td>\n",
       "      <td>130069</td>\n",
       "      <td>Chat, cheval, dinosaures: des élèves s’identif...</td>\n",
       "      <td>blog</td>\n",
       "      <td>TRADFEM</td>\n",
       "      <td>21/06/2023</td>\n",
       "      <td>Chat, cheval, dinosaures: des élèves s’identif...</td>\n",
       "      <td>https://tradfem.wordpress.com/2023/06/21/chat-...</td>\n",
       "      <td>chat cheval dinosaure élève s’ identifier anim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>23</td>\n",
       "      <td>230005</td>\n",
       "      <td>\\r\\nENTRETIEN. À l’heure où l’Espagne vient d’...</td>\n",
       "      <td>Entretien</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.observatoirepetitesirene.org/post/...</td>\n",
       "      <td>\\r\\n entretien l’ heure l’ espagne d’ adopter ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>22</td>\n",
       "      <td>220003</td>\n",
       "      <td>« La médecine face à la transidentité de genre...</td>\n",
       "      <td>Communiqué</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-02-28 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>médecine face transidentité genre enfant adole...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>13</td>\n",
       "      <td>130026</td>\n",
       "      <td>Le viol punitif : Violence envers les lesbienn...</td>\n",
       "      <td>blog</td>\n",
       "      <td>TRADFEM</td>\n",
       "      <td>12/09/2023</td>\n",
       "      <td>Le viol punitif : Violence envers les lesbiennes</td>\n",
       "      <td>https://tradfem.wordpress.com/2023/09/12/le-vi...</td>\n",
       "      <td>viol punitif violence envers lesbiennestradfem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>21</td>\n",
       "      <td>210014</td>\n",
       "      <td>Traduction principalement DeepL de cet article...</td>\n",
       "      <td>analyse</td>\n",
       "      <td>résistance lesbienne</td>\n",
       "      <td>2021-12-20</td>\n",
       "      <td>Changement de définition</td>\n",
       "      <td>NaN</td>\n",
       "      <td>traduction principalement deepl article écrire...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ID ASSO  ID ARTICLE                                            contenu  \\\n",
       "439       13      130069  Chat, cheval, dinosaures: des élèves s’identif...   \n",
       "230       23      230005  \\r\\nENTRETIEN. À l’heure où l’Espagne vient d’...   \n",
       "318       22      220003  « La médecine face à la transidentité de genre...   \n",
       "396       13      130026  Le viol punitif : Violence envers les lesbienn...   \n",
       "144       21      210014  Traduction principalement DeepL de cet article...   \n",
       "\n",
       "    Type de document                Auteur                 Date  \\\n",
       "439             blog               TRADFEM           21/06/2023   \n",
       "230        Entretien                   NaN                  NaN   \n",
       "318       Communiqué                   NaN  2022-02-28 00:00:00   \n",
       "396             blog               TRADFEM           12/09/2023   \n",
       "144          analyse  résistance lesbienne           2021-12-20   \n",
       "\n",
       "                                                 Titre  \\\n",
       "439  Chat, cheval, dinosaures: des élèves s’identif...   \n",
       "230                                                NaN   \n",
       "318                                                NaN   \n",
       "396   Le viol punitif : Violence envers les lesbiennes   \n",
       "144                           Changement de définition   \n",
       "\n",
       "                                                   URL  \\\n",
       "439  https://tradfem.wordpress.com/2023/06/21/chat-...   \n",
       "230  https://www.observatoirepetitesirene.org/post/...   \n",
       "318                                                NaN   \n",
       "396  https://tradfem.wordpress.com/2023/09/12/le-vi...   \n",
       "144                                                NaN   \n",
       "\n",
       "                                    lemmatized_contenu  \n",
       "439  chat cheval dinosaure élève s’ identifier anim...  \n",
       "230  \\r\\n entretien l’ heure l’ espagne d’ adopter ...  \n",
       "318  médecine face transidentité genre enfant adole...  \n",
       "396  viol punitif violence envers lesbiennestradfem...  \n",
       "144  traduction principalement deepl article écrire...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the French language model\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "# Define your list of words to exclude\n",
    "with open(\"../data/intermediate/words_to_filter.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    words_to_filter = file.read().splitlines()\n",
    "\n",
    "word_meaning = []\n",
    "\n",
    "# Function to tokenize, filter, and lemmatize text, excluding numbers\n",
    "def tokenize_filter_and_lemmatize(text):\n",
    "    # Traite le texte avec spaCy\n",
    "    doc = nlp(text)\n",
    "    # Initialise une liste pour le résultat lemmatisé\n",
    "    lemmatized_tokens = []\n",
    "\n",
    "    for token in doc:\n",
    "        # Vérifie si le token est \"trans\", sinon applique la lemmatisation et le filtre\n",
    "        if token.text == \"trans\":\n",
    "            lemmatized_tokens.append(token.text)\n",
    "        elif token.lemma_ not in words_to_filter and not token.is_digit:\n",
    "            lemmatized_tokens.append(token.lemma_)\n",
    "        \n",
    "        if token.lemma_ == \"tran\":\n",
    "            word_meaning.append(token.text)\n",
    "\n",
    "    # Joint les tokens lemmatisés en une chaîne de texte\n",
    "    lemmatized = \" \".join(lemmatized_tokens)\n",
    "    return lemmatized\n",
    "\n",
    "# Apply the function to the 'contenu' column\n",
    "df['lemmatized_contenu'] = df['contenu'].progress_apply(tokenize_filter_and_lemmatize)\n",
    "\n",
    "# Print the dataframe to see the result\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['TRANS', 'Trans', 'tran', 'trans'], dtype='<U5')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.unique(word_meaning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking Words frequency according to Spacy tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import spacy\n",
    "\n",
    "if compute_freq:\n",
    "    # Load the French language model\n",
    "    nlp = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "    # Your list of words to filter and the tokenize_filter_and_lemmatize function should be defined here\n",
    "\n",
    "    # Initialize a Counter object to hold the frequency of each token\n",
    "    token_freq = Counter()\n",
    "\n",
    "    # Function to update token frequency for a single document\n",
    "    def update_token_frequency(text):\n",
    "        global token_freq  # Reference the global Counter object\n",
    "        # Process the text with spaCy\n",
    "        doc = nlp(text)\n",
    "        # Update the Counter with tokens from this document, excluding filtered words and numbers\n",
    "        token_freq.update([token.text for token in doc if token.lemma_ not in words_to_filter and not token.is_digit])\n",
    "\n",
    "    # Apply the function to each row in the 'lemmatized_contenu' column to update the global token frequency\n",
    "    df['lemmatized_contenu'].progress_apply(update_token_frequency)\n",
    "\n",
    "    # Find the top 10 most frequent tokens\n",
    "    top_10_tokens = token_freq.most_common(10)\n",
    "\n",
    "    # Print the top 10 most frequent tokens\n",
    "    print(\"Top 10 most frequent tokens:\")\n",
    "    for token, freq in top_10_tokens:\n",
    "        print(f\"{token}: {freq}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second filtering with nltk\n",
    "\n",
    "the tokenization of spacy is better than the one I used in R (we do the LDA in R). The one in R is equivalent to the one of nltk. And for instance aujourd'hui appears as aujourd + ' + hui. So i need to run a second filter based on nltk tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\leopo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb6d806b22d94c4d9f06feebe1e51867",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure NLTK resources are downloaded (needed for tokenization)\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Function to tokenize with NLTK and filter based on your criteria\n",
    "def nltk_tokenize_and_filter(text):\n",
    "    # Tokenize the text with NLTK, ensuring the text is treated as French\n",
    "    tokens = word_tokenize(text, language='french')\n",
    "    # Filter tokens: convert to lowercase, exclude if in words_to_filter or is a digit\n",
    "    filtered_tokens = [token.lower() for token in tokens if token.lower() not in words_to_filter and not token.isdigit()]\n",
    "    # Join the filtered tokens back into a string\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    return filtered_text\n",
    "\n",
    "# Assuming 'df' is your DataFrame and 'lemmatized_contenu' contains the text to process\n",
    "# Apply the function to each row in the 'lemmatized_contenu' column\n",
    "df['lemmatized_contenu'] = df['lemmatized_contenu'].progress_apply(nltk_tokenize_and_filter)\n",
    "\n",
    "# This results in a new column 'filtered_contenu' in your dataframe 'df' with the text processed as per your requirements\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking words frequency based on nltk tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "if compute_freq:\n",
    "    # Ensure you've downloaded the NLTK tokenizer models\n",
    "    nltk.download('punkt')\n",
    "\n",
    "    # Your DataFrame 'df' should already be loaded with the 'lemmatized_contenu' column ready\n",
    "\n",
    "    # Initialize a Counter object for token frequencies\n",
    "    token_freq = Counter()\n",
    "\n",
    "    # Define your list of words to exclude, adjusted for your context\n",
    "    words_to_filter = set([\n",
    "        # Your list of words to filter\n",
    "    ])\n",
    "\n",
    "    # Function to tokenize and update token frequency for a single document using NLTK\n",
    "    def update_token_frequency_nltk(text):\n",
    "        global token_freq  # Reference the global Counter object\n",
    "        # Tokenize the text using NLTK\n",
    "        tokens = word_tokenize(text, language='french')\n",
    "        # Update the Counter with tokens from this document, excluding filtered words and numbers\n",
    "        token_freq.update([token.lower() for token in tokens if token.lower() not in words_to_filter and not token.isdigit()])\n",
    "\n",
    "    # Apply the function to each row in the 'lemmatized_contenu' column to update the global token frequency\n",
    "    df['lemmatized_contenu'].apply(update_token_frequency_nltk)\n",
    "\n",
    "    # Find the top 10 most frequent tokens\n",
    "    top_10_tokens = token_freq.most_common(10)\n",
    "\n",
    "    # Print the top 10 most frequent tokens\n",
    "    print(\"Top 10 most frequent tokens using NLTK tokenization:\")\n",
    "    for token, freq in top_10_tokens:\n",
    "        print(f\"{token}: {freq}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "fichier = \"../data/intermediate/base_lemmatized.csv\"\n",
    "df.to_csv(fichier, sep=';', index=False, quotechar='\"')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ccne",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
