{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0f74da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #On va importer les packages \n",
    "# !pip install wordcloud \n",
    "# !pip install nltk\n",
    "# !pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22f957dd-0b41-4614-a208-c48f9a960f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\leopo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\leopo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\leopo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('french'))\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "from gensim import corpora, models\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6913801a-e65d-4054-bb1d-a5a9b9d657f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID ASSO</th>\n",
       "      <th>ID ARTICLE</th>\n",
       "      <th>contenu</th>\n",
       "      <th>Type de document</th>\n",
       "      <th>Auteur</th>\n",
       "      <th>Date</th>\n",
       "      <th>Titre</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>30</td>\n",
       "      <td>300011</td>\n",
       "      <td>\\r\\nLe 14 juillet dernier, à 20h22, Trystan, p...</td>\n",
       "      <td>Article d'opinion</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-07-17 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.parti-de-la-france.fr/articles/On-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>13</td>\n",
       "      <td>130054</td>\n",
       "      <td>La Suédoise FRIDA STRANNE défend son plus réce...</td>\n",
       "      <td>blog</td>\n",
       "      <td>TRADFEM</td>\n",
       "      <td>16/07/2023</td>\n",
       "      <td>La Suédoise FRIDA STRANNE défend son plus réce...</td>\n",
       "      <td>https://tradfem.wordpress.com/2023/07/16/la-su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>13</td>\n",
       "      <td>130053</td>\n",
       "      <td>L’Alliance LGB ne sera jamais réduite au silen...</td>\n",
       "      <td>blog</td>\n",
       "      <td>TRADFEM</td>\n",
       "      <td>16/07/2023</td>\n",
       "      <td>L’Alliance LGB ne sera jamais réduite au silence</td>\n",
       "      <td>https://tradfem.wordpress.com/2023/07/16/lalli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>13</td>\n",
       "      <td>130034</td>\n",
       "      <td>CANADA : Quand le transactivisme devient une p...</td>\n",
       "      <td>blog</td>\n",
       "      <td>TRADFEM</td>\n",
       "      <td>26/08/2023</td>\n",
       "      <td>CANADA : Quand le transactivisme devient une p...</td>\n",
       "      <td>https://tradfem.wordpress.com/2023/08/26/canad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>20</td>\n",
       "      <td>200093</td>\n",
       "      <td>De nombreux enfants souffrent du sentiment qu’...</td>\n",
       "      <td>Guide</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://cryforrecognition.be/fr/guide-concis-p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ID ASSO  ID ARTICLE                                            contenu  \\\n",
       "252       30      300011  \\r\\nLe 14 juillet dernier, à 20h22, Trystan, p...   \n",
       "346       13      130054  La Suédoise FRIDA STRANNE défend son plus réce...   \n",
       "345       13      130053  L’Alliance LGB ne sera jamais réduite au silen...   \n",
       "326       13      130034  CANADA : Quand le transactivisme devient une p...   \n",
       "231       20      200093  De nombreux enfants souffrent du sentiment qu’...   \n",
       "\n",
       "      Type de document   Auteur                 Date  \\\n",
       "252  Article d'opinion      NaN  2018-07-17 00:00:00   \n",
       "346               blog  TRADFEM           16/07/2023   \n",
       "345               blog  TRADFEM           16/07/2023   \n",
       "326               blog  TRADFEM           26/08/2023   \n",
       "231              Guide      NaN                  NaN   \n",
       "\n",
       "                                                 Titre  \\\n",
       "252                                                NaN   \n",
       "346  La Suédoise FRIDA STRANNE défend son plus réce...   \n",
       "345   L’Alliance LGB ne sera jamais réduite au silence   \n",
       "326  CANADA : Quand le transactivisme devient une p...   \n",
       "231                                                NaN   \n",
       "\n",
       "                                                   URL  \n",
       "252  https://www.parti-de-la-france.fr/articles/On-...  \n",
       "346  https://tradfem.wordpress.com/2023/07/16/la-su...  \n",
       "345  https://tradfem.wordpress.com/2023/07/16/lalli...  \n",
       "326  https://tradfem.wordpress.com/2023/08/26/canad...  \n",
       "231  https://cryforrecognition.be/fr/guide-concis-p...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Récupérations données \n",
    "base_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "# Construisez le chemin complet vers le répertoire de données\n",
    "fichier = os.path.join(base_dir, 'data','intermediate', 'base_merged.csv')\n",
    "\n",
    "df = pd.read_csv(fichier, sep=';', quotechar='\"')\n",
    "\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ddf0a0c-6c0d-4de2-82da-ea3ab72156fe",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 39\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     37\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m word\n\u001b[0;32m---> 39\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontenu\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcontenu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43msingularize_word\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mword\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m all_text2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontenu\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Fonction pour obtenir les mots les plus représentés\u001b[39;00m\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.11/site-packages/pandas/core/series.py:4904\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4769\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4770\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4771\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4776\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4777\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4778\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4779\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4780\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4895\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4896\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   4897\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4898\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4899\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4900\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4901\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4902\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4903\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m-> 4904\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.11/site-packages/pandas/core/apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[0;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.11/site-packages/pandas/core/apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[1;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[1;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.11/site-packages/pandas/core/base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.11/site-packages/pandas/core/algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[1;32m   1747\u001b[0m     )\n",
      "File \u001b[0;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[9], line 39\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     37\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m word\n\u001b[0;32m---> 39\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontenu\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontenu\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[43m[\u001b[49m\u001b[43msingularize_word\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mword\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m))\n\u001b[1;32m     41\u001b[0m all_text2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontenu\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Fonction pour obtenir les mots les plus représentés\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[9], line 39\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     37\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m word\n\u001b[0;32m---> 39\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontenu\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontenu\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[43msingularize_word\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m x\u001b[38;5;241m.\u001b[39msplit()]))\n\u001b[1;32m     41\u001b[0m all_text2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontenu\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Fonction pour obtenir les mots les plus représentés\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[9], line 34\u001b[0m, in \u001b[0;36msingularize_word\u001b[0;34m(word)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msingularize_word\u001b[39m(word):\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m# Retire le 's' à la fin du mot si présent et si la version sans 's' existe déjà strictement\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m word\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m word[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mset\u001b[39m(all_text\u001b[38;5;241m.\u001b[39msplit()):\n\u001b[1;32m     35\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m word[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Travail des stop words\n",
    "mots_fr = [\n",
    "    'le', 'la', 'les', 'de', 'un', 'à', 'être', 'suis', 'es', 'est', 'sont', 'été', 'étais', 'était', 'étaient', 'serai', 'seras', 'sera', 'seront',\n",
    "    'et', 'en', 'avoir', 'ai', 'as', 'a', 'ont', 'eu', 'aurai', 'auras', 'aura', 'auront', 'avais', 'avait', 'avaient', 'que', 'pour',\n",
    "    'dans', 'ce', 'il', 'qui', 'ne', 'en', 'sur', 'se', 'pas', 'plus',\n",
    "    'par', 'je', 'avec', 'tout', 'faire', 'fais', 'fait', 'font', 'son', 'mettre', 'autre', 'on', 'mais',\n",
    "    'nous', 'comme', 'ou', 'si', 'leur', 'y', 'dire', 'dis', 'dit', 'disent' 'elle', 'voir', 'devoir',\n",
    "    'deux', 'même', 'pendant', 'aussi', 'vouloir', 'grand', 'mon', 'nouveau', 'aller',\n",
    "    'venir', 'ceux', 'faire', 'quelque', 'trouver', 'donner', 'donne', 'donnes', 'donnent', 'aussi', 'autre',\n",
    "    'tous', 'vieux', 'bon', 'voir', 'moins', 'trois', 'avant', 'sa', 'faire', 'contre',\n",
    "    'abord', 'sous', 'ou', 'apporter', 'grand', 'ainsi', 'long', 'très', 'tout', 'avoir',\n",
    "    'beau', 'chaque', 'peu', 'quoi', 'encore', 'aller', 'montrer', 'semaine', 'ainsi',\n",
    "    'nuit', 'aussi', 'bien', 'deuxième', 'moins', 'tout', 'avoir', 'peu', 'nuit', 'ça',\n",
    "    'mon', 'ma', 'mes', 'ton', 'ta', 'tes', 'son', 'sa', 'ses', 'notre', 'notre', 'nos', 'votre', 'votre', 'vos', 'leur', 'leur', 'leurs',\n",
    "    'non', '’', '.', ',', ':', 'est', 'des', \"l'\", \"d'\", \"une\", \"d\", \"»\", \"«\", \"du\", \"qu\", \"au\", \"n\", \"s\", \"elle\", \")\", \"(\", \"aux\", 'c', 'cette', 'un', 'ces','?', 'il', 'ils', 'j', \"l\", \"-\", \"vous\", \"entre\",\n",
    "    \"[\", \"]\", \"elles\", '”', '“', \"dont\", \"cela\", \";\", \"m\", \"selon\", \"!\", '–', \"en\", \"autre\", \"ici\", \"ce\", \"ça\", \"cela\", \"ceci\", \"car\", \"ou\", \"où\", \"ni\", \"mais\", \"et\", \"donc\", \"parce\", \"que\", \"quand\", \"comment\", \"qui\", \"quoi\",\n",
    "    \"à\", \"de\", \"pour\", \"sans\", \"sur\"\n",
    "]\n",
    "# Concaténer tous les textes de la colonne 'contenu' en une seule chaîne\n",
    "all_text = ' '.join(df['contenu'])\n",
    "\n",
    "# Fonction pour enlever les mots de la liste mots_fr\n",
    "def remove_words(text):\n",
    "    words = word_tokenize(text.lower())  # normaliser la casse ici\n",
    "    filtered_words = [word for word in words if word not in mots_fr]\n",
    "    return ' '.join(filtered_words)\n",
    "    \n",
    "# Appliquer la fonction remove_words\n",
    "df['contenu'] = df['contenu'].apply(lambda x: remove_words(x))\n",
    "\n",
    "# Fonction pour normaliser les mots au singulier \n",
    "def singularize_word(word):\n",
    "    all_words = set(all_text.split())\n",
    "    # Retire le 's' à la fin du mot si présent et si la version sans 's' existe déjà strictement\n",
    "    if word.lower().endswith('s') and word[:-1].lower() in all_words:\n",
    "        return word[:-1]\n",
    "    #Pluriels en X\n",
    "    elif word.lower().endswith('x') and word[:-1].lower() in all_words:\n",
    "        return word[:-1]\n",
    "    #mots en al\n",
    "    elif word.lower().endswith('x') and (word[:-2].lower() + 'l') in all_words:\n",
    "        return word[:-2].lower() + 'l'\n",
    "    #mots en ail \n",
    "    elif word.lower().endswith('x') and (word[:-2].lower() + 'il') in all_words:\n",
    "        return word[:-2].lower() + 'il'\n",
    "    else:\n",
    "        return word\n",
    "        \n",
    "df['contenu'] = df['contenu'].apply(lambda x: ' '.join([singularize_word(word) for word in x.split()]))\n",
    "\n",
    "all_text2 = ' '.join(df['contenu'])\n",
    "\n",
    "# Fonction pour obtenir les mots les plus représentés\n",
    "def get_most_common_words(text):\n",
    "    words = word_tokenize(text.lower())\n",
    "    freq_dist = FreqDist(words)\n",
    "    return freq_dist.most_common()\n",
    "\n",
    "# Appliquer la fonction à l'ensemble du texte\n",
    "most_common_words = get_most_common_words(all_text2)\n",
    "most_common_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce395f1-9791-4599-b73e-cf8a81f66c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "mots_fr2 = [\"m.\", '—', '``', \"autre\", \"peut\", \"peuvent\", \"ont\", \"a\", \"suis\", \"es\", \"sont\", \"sommes\", \"êtes\", \"me\", \"te\", \"se\", \"tant\", \"alors\", \"toute\", \"tout\", \"tous\", \"toutes\", \"façon\", \"chose\", \"aujourd\", \"hui\",\n",
    "           \"façon\", \"soit\", \"lui\", \"moi\"]\n",
    "\n",
    "def remove_words2(text):\n",
    "    words = word_tokenize(text.lower())  # normaliser la casse ici\n",
    "    filtered_words = [word for word in words if word not in mots_fr2]\n",
    "    return ' '.join(filtered_words)\n",
    "    \n",
    "# Appliquer la fonction remove_words\n",
    "df['contenu'] = df['contenu'].apply(lambda x: remove_words2(x))\n",
    "\n",
    "all_text2 = ' '.join(df['contenu'])\n",
    "\n",
    "most_common_words = get_most_common_words(all_text2)\n",
    "most_common_words[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "90f8cc70-a808-4dc3-aa19-a2bd6a6bfdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pour corriger le genre \n",
    "def masc_word(word):\n",
    "    # Retire le 'e' à la fin du mot si présent et si la version sans 'e' existe déjà strictement\n",
    "    if word.lower().endswith('e') and word[:-1].lower() in set(all_text.split()):\n",
    "        return word[:-1]\n",
    "    # Adaptation au cas des mots en -er (ex : berger --> bergère)\n",
    "    elif word.lower().endswith('re') and word[-3].lower() in ['e', 'è', 'é'] and (word[:-3].lower() + 'er') in set(all_text.split()):\n",
    "        return word[:-3] + 'er'\n",
    "    #Cas de mots en f et p (loup --> louve, veuf --> veuve) \n",
    "    elif (word.lower().endswith('ve') and ((word[-2].lower() + 'f') in set(all_text.split()) or (word[-2].lower() + 'p') in set(all_text.split()))):\n",
    "        if word[-2] + 'f' in set(all_text.split()): \n",
    "            return word[-2] + 'f'\n",
    "        if word[-2] + 'p' in set(all_text.split()): \n",
    "            return word[-2] + 'p'\n",
    "    #Cas de la double consonne \n",
    "    elif word.lower().endswith('e') and word[-2] == word[-3] and word[:-2].lower() in set(all_text.split()):\n",
    "        return word[:-2]\n",
    "    #chameau --> Chamelle \n",
    "    elif word.lower().endswith('elle') and (word[:-4].lower() + 'eau') in set(all_text.split()):\n",
    "        return word[:-4] + 'eau'\n",
    "    #époux --> Epouse \n",
    "    elif word.lower().endswith('se') and (word[:-2].lower() + 'x') in set(all_text.split()):\n",
    "        return word[:-2] + 'x' \n",
    "    else:\n",
    "       \n",
    "df['contenu'] = df['contenu'].apply(lambda x: ' '.join([singularize_word(word) for word in x.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8ca4b58-a1e8-4aa6-9da2-5db6f6f4c5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenez le chemin du répertoire parent du répertoire src\n",
    "base_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "# Construisez le chemin complet vers le répertoire de données\n",
    "chemin_data = os.path.join(base_dir, 'data')\n",
    "\n",
    "# Enregistrez le DataFrame dans un fichier CSV dans le répertoire data\n",
    "df.to_csv(os.path.join(chemin_data, 'base_clean.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5298e7b1-1c10-4943-900b-70dd4fa293b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Récupérations données \n",
    "base_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "# Construisez le chemin complet vers le répertoire de données\n",
    "fichier = os.path.join(base_dir, 'data', 'base_clean.csv')\n",
    "\n",
    "df = pd.read_csv(fichier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ad34323-6694-4bdf-bd77-e4e1b7173efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenisation et suppression des stop words\n",
    "stop_words = set(stopwords.words('french'))\n",
    "df['tokenized_content'] = df['contenu'].apply(lambda x: [word for word in simple_preprocess(x) if word not in stop_words])\n",
    "\n",
    "# Création d'un dictionnaire et d'un corpus\n",
    "dictionary = corpora.Dictionary(df['tokenized_content'])\n",
    "corpus = [dictionary.doc2bow(doc) for doc in df['tokenized_content']]\n",
    "\n",
    "# Entraînement du modèle LDA\n",
    "lda_model = models.LdaModel(corpus, num_topics=25, id2word=dictionary, passes=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "027ce57c-6e9f-47be-9f57-fac784fa7b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic #15: 0.032*\"femelle\" + 0.022*\"mâle\" + 0.014*\"humanité\" + 0.012*\"corps\" + 0.012*\"rapport\" + 0.010*\"vie\" + 0.010*\"violence\" + 0.009*\"pouvoir\" + 0.009*\"humaine\" + 0.009*\"sexuation\" + 0.009*\"puissance\" + 0.009*\"réel\" + 0.008*\"décolonisation\" + 0.008*\"monde\" + 0.008*\"femme\"\n",
      "\n",
      "Topic #3: 0.007*\"alliance\" + 0.006*\"mermaids\" + 0.005*\"lgb\" + 0.004*\"transactiviste\" + 0.004*\"travesti\" + 0.004*\"pancarte\" + 0.004*\"organisation\" + 0.003*\"passé\" + 0.003*\"clinique\" + 0.003*\"promotion\" + 0.003*\"commission\" + 0.003*\"caritative\" + 0.003*\"ajouté\" + 0.003*\"juridique\" + 0.003*\"police\"\n",
      "\n",
      "Topic #12: 0.012*\"violence\" + 0.010*\"femme\" + 0.010*\"hanmer\" + 0.005*\"féministe\" + 0.005*\"homme\" + 0.004*\"royaume\" + 0.004*\"domestique\" + 0.004*\"uni\" + 0.004*\"universitaire\" + 0.003*\"nationale\" + 0.003*\"association\" + 0.003*\"tre\" + 0.003*\"women\" + 0.003*\"année\" + 0.003*\"enfant\"\n",
      "\n",
      "Topic #22: 0.011*\"femme\" + 0.010*\"chromosome\" + 0.009*\"sexe\" + 0.008*\"sexuel\" + 0.007*\"genre\" + 0.006*\"chez\" + 0.006*\"homme\" + 0.005*\"victime\" + 0.005*\"ensemble\" + 0.005*\"personne\" + 0.005*\"délinquant\" + 0.005*\"police\" + 0.004*\"système\" + 0.004*\"humain\" + 0.004*\"crime\"\n",
      "\n",
      "Topic #5: 0.019*\"droit\" + 0.012*\"gaza\" + 0.010*\"israël\" + 0.009*\"palestinien\" + 0.007*\"unies\" + 0.007*\"nation\" + 0.006*\"devons\" + 0.006*\"peuple\" + 0.005*\"somme\" + 0.005*\"israélien\" + 0.005*\"palestine\" + 0.005*\"haut\" + 0.005*\"mokhiber\" + 0.004*\"hôpital\" + 0.004*\"génocide\"\n",
      "\n",
      "Topic #8: 0.000*\"homme\" + 0.000*\"femme\" + 0.000*\"personne\" + 0.000*\"genre\" + 0.000*\"droit\" + 0.000*\"enfant\" + 0.000*\"sexe\" + 0.000*\"sexuelle\" + 0.000*\"lesbienne\" + 0.000*\"trans\" + 0.000*\"an\" + 0.000*\"mouvement\" + 0.000*\"identité\" + 0.000*\"année\" + 0.000*\"féministe\"\n",
      "\n",
      "Topic #21: 0.018*\"genre\" + 0.013*\"femme\" + 0.009*\"féministe\" + 0.008*\"homme\" + 0.007*\"ny\" + 0.006*\"musée\" + 0.005*\"critique\" + 0.004*\"pourquoi\" + 0.004*\"sexe\" + 0.004*\"libération\" + 0.004*\"radicale\" + 0.003*\"institution\" + 0.003*\"question\" + 0.003*\"personne\" + 0.003*\"rôle\"\n",
      "\n",
      "Topic #19: 0.016*\"femme\" + 0.009*\"homme\" + 0.005*\"violence\" + 0.005*\"enfant\" + 0.004*\"pouvoir\" + 0.004*\"corps\" + 0.004*\"brand\" + 0.003*\"monde\" + 0.003*\"pourquoi\" + 0.003*\"propre\" + 0.003*\"vie\" + 0.003*\"là\" + 0.003*\"idée\" + 0.003*\"nommer\" + 0.003*\"question\"\n",
      "\n",
      "Topic #11: 0.016*\"femme\" + 0.010*\"terf\" + 0.009*\"homme\" + 0.006*\"genre\" + 0.006*\"critique\" + 0.005*\"féministe\" + 0.004*\"différence\" + 0.004*\"féminisme\" + 0.004*\"transgenre\" + 0.004*\"chez\" + 0.004*\"terme\" + 0.004*\"organe\" + 0.003*\"position\" + 0.003*\"celle\" + 0.003*\"marguerite\"\n",
      "\n",
      "Topic #24: 0.013*\"genre\" + 0.010*\"homme\" + 0.010*\"femme\" + 0.009*\"eglise\" + 0.007*\"sexuelle\" + 0.006*\"enfant\" + 0.006*\"sexe\" + 0.006*\"éducation\" + 0.005*\"différence\" + 0.005*\"famille\" + 0.005*\"question\" + 0.005*\"loi\" + 0.005*\"document\" + 0.005*\"identité\" + 0.004*\"société\"\n",
      "\n",
      "Topic #1: 0.007*\"florida\" + 0.006*\"lesbienne\" + 0.004*\"pénis\" + 0.004*\"grattige\" + 0.004*\"transgenrisme\" + 0.003*\"homophobie\" + 0.003*\"ritson\" + 0.003*\"get\" + 0.003*\"forme\" + 0.003*\"refusent\" + 0.002*\"but\" + 0.002*\"white\" + 0.002*\"orientation\" + 0.002*\"transidentifié\" + 0.002*\"lesbians\"\n",
      "\n",
      "Topic #23: 0.000*\"femme\" + 0.000*\"homme\" + 0.000*\"genre\" + 0.000*\"sexe\" + 0.000*\"personne\" + 0.000*\"enfant\" + 0.000*\"féministe\" + 0.000*\"droit\" + 0.000*\"an\" + 0.000*\"jeune\" + 0.000*\"monde\" + 0.000*\"violence\" + 0.000*\"trans\" + 0.000*\"politique\" + 0.000*\"question\"\n",
      "\n",
      "Topic #6: 0.011*\"intersexuation\" + 0.008*\"intersexe\" + 0.004*\"gamète\" + 0.002*\"instrumentalise\" + 0.001*\"église\" + 0.001*\"propagées\" + 0.001*\"légion\" + 0.001*\"bases\" + 0.001*\"podcastrebelles\" + 0.001*\"immobiles\" + 0.001*\"aberrant\" + 0.001*\"évolutifs\" + 0.001*\"standardisation\" + 0.001*\"paralicenotre\" + 0.001*\"mensongères\"\n",
      "\n",
      "Topic #0: 0.013*\"féminicide\" + 0.013*\"femme\" + 0.009*\"maternité\" + 0.009*\"substitution\" + 0.008*\"loi\" + 0.007*\"mère\" + 0.007*\"personne\" + 0.007*\"an\" + 0.006*\"porteuse\" + 0.006*\"discrimination\" + 0.005*\"droit\" + 0.004*\"sexe\" + 0.004*\"patiente\" + 0.004*\"commission\" + 0.004*\"soin\"\n",
      "\n",
      "Topic #13: 0.017*\"prostitution\" + 0.012*\"connor\" + 0.011*\"sinéad\" + 0.008*\"sexe\" + 0.007*\"personne\" + 0.006*\"travail\" + 0.005*\"sexuelle\" + 0.005*\"pape\" + 0.005*\"sexuel\" + 0.005*\"chanson\" + 0.005*\"consentement\" + 0.005*\"rachel\" + 0.004*\"musique\" + 0.003*\"françois\" + 0.003*\"emploi\"\n",
      "\n",
      "Topic #18: 0.016*\"filia\" + 0.007*\"maternité\" + 0.006*\"substitution\" + 0.006*\"femme\" + 0.005*\"féministe\" + 0.005*\"australienne\" + 0.004*\"abolition\" + 0.004*\"loi\" + 0.004*\"org\" + 0.003*\"www\" + 0.003*\"droit\" + 0.003*\"https\" + 0.003*\"homme\" + 0.003*\"uk\" + 0.003*\"surrogacy\"\n",
      "\n",
      "Topic #9: 0.021*\"stock\" + 0.012*\"oxford\" + 0.010*\"kathleen\" + 0.010*\"femme\" + 0.008*\"université\" + 0.006*\"étudiant\" + 0.006*\"sexe\" + 0.006*\"personne\" + 0.006*\"professeure\" + 0.005*\"droit\" + 0.004*\"transgenre\" + 0.004*\"statistique\" + 0.004*\"islington\" + 0.004*\"union\" + 0.004*\"débat\"\n",
      "\n",
      "Topic #7: 0.022*\"femme\" + 0.012*\"homme\" + 0.007*\"genre\" + 0.007*\"sexe\" + 0.006*\"personne\" + 0.005*\"droit\" + 0.004*\"enfant\" + 0.004*\"féministe\" + 0.004*\"trans\" + 0.003*\"identité\" + 0.003*\"sexuelle\" + 0.003*\"jeune\" + 0.003*\"violence\" + 0.002*\"politique\" + 0.002*\"idéologie\"\n",
      "\n",
      "Topic #17: 0.014*\"femme\" + 0.006*\"sexe\" + 0.006*\"enfant\" + 0.006*\"prostitution\" + 0.005*\"noire\" + 0.004*\"adoption\" + 0.004*\"association\" + 0.004*\"personne\" + 0.003*\"politique\" + 0.003*\"genre\" + 0.003*\"commerce\" + 0.003*\"question\" + 0.003*\"école\" + 0.003*\"raciste\" + 0.003*\"industrie\"\n",
      "\n",
      "Topic #10: 0.007*\"chien\" + 0.004*\"personne\" + 0.004*\"humain\" + 0.004*\"kaz\" + 0.003*\"identité\" + 0.003*\"toujours\" + 0.003*\"ami\" + 0.003*\"james\" + 0.003*\"senti\" + 0.002*\"genre\" + 0.002*\"an\" + 0.002*\"réalité\" + 0.002*\"jamais\" + 0.002*\"âge\" + 0.002*\"théorie\"\n"
     ]
    }
   ],
   "source": [
    "# Fonction d'affichage\n",
    "def print_topics(model, n_top_words):\n",
    "    topics = model.print_topics(num_words=n_top_words)\n",
    "    for topic_idx, topic in topics:\n",
    "        print(f\"\\nTopic #{topic_idx}: {topic}\")\n",
    "\n",
    "# Affichage des résultats\n",
    "print_topics(lda_model, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f5607f-6cc4-4355-a54c-35b50512b5ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
